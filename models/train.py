{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7624bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# from dataset import IsoNetDataset\n",
    "import os \n",
    "\n",
    "# --- CONFIGURATION FROM DIAGRAM ---\n",
    "VISUAL_DIM = 256       # Output of Visual Stream (V)\n",
    "SPATIAL_DIM = 128      # Output of Spatial Stream (S)\n",
    "AUDIO_ENC_DIM = 512    # Internal Audio Feature Dimension\n",
    "AUDIO_CHANNELS = 4     # Number of Mics\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4          # FIXED: Increased from 1 to 4 for proper BatchNorm\n",
    "EPOCHS = 100\n",
    "LR = 1e-4               # TCNs prefer lower learning rates\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6852e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()        # free cached memory\n",
    "torch.cuda.synchronize()        # wait for all kernels to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37930781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATASET PATHS CONFIGURATION ---\n",
    "import platform\n",
    "\n",
    "# Detect OS and set paths accordingly\n",
    "IS_WINDOWS = platform.system() == \"Windows\"\n",
    "\n",
    "# Windows paths\n",
    "WINDOWS_PATHS = {\n",
    "    \"root_dir\": \"E:/Dataset/VOX/manual\",\n",
    "    \"data_dir\": \"E:/Dataset/VOX/manual/dev\",\n",
    "    \"bbox_dir\": \"E:/Dataset/VOX/manual/vox2_dev_txt/txt\",\n",
    "    \"train_csv\": \"E:/Dataset/VOX/manual/dev/multich/train.csv\",\n",
    "    \"val_csv\": \"E:/Dataset/VOX/manual/dev/multich/val.csv\",\n",
    "}\n",
    "\n",
    "# Linux paths\n",
    "LINUX_PATHS = {\n",
    "    \"root_dir\": \"/mnt/DATA/Bibek/Speech/isolate-speech\",\n",
    "    \"data_dir\": \"/mnt/DATA/Bibek/Speech/isolate-speech/data\",\n",
    "    \"bbox_dir\": \"/mnt/DATA/Bibek/Speech/isolate-speech/data/vox2_dev_txt/txt\",\n",
    "    \"train_csv\": \"/mnt/DATA/Bibek/Speech/isolate-speech/data/multich/train.csv\",\n",
    "    \"val_csv\": \"/mnt/DATA/Bibek/Speech/isolate-speech/data/multich/val.csv\",\n",
    "}\n",
    "\n",
    "# Select paths based on OS\n",
    "PATHS = WINDOWS_PATHS if IS_WINDOWS else LINUX_PATHS\n",
    "\n",
    "ROOT_DIR = PATHS[\"root_dir\"]\n",
    "DATA_DIR = PATHS[\"data_dir\"]\n",
    "BBOX_DIR = PATHS[\"bbox_dir\"]\n",
    "TRAIN_CSV = PATHS[\"train_csv\"]\n",
    "VAL_CSV = PATHS[\"val_csv\"]\n",
    "\n",
    "print(f\"OS: {platform.system()}\")\n",
    "print(f\"Using paths: {'Windows' if IS_WINDOWS else 'Linux'}\")\n",
    "print(f\"Root Dir: {ROOT_DIR}\")\n",
    "print(f\"BBox Dir: {BBOX_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d726b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IsoNetDataset(Dataset):\n",
    "    def __init__(self, csv_path, clip_length=4.0, fps=25, video_size=(224, 224), max_samples=None, root_dir=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (str): Path to train.csv or val.csv\n",
    "            clip_length (float): Audio duration in seconds (must match simulation)\n",
    "            fps (int): Target frames per second for video (VoxCeleb is 25)\n",
    "            video_size (tuple): Target resize dimension (H, W) - 224x224 for ResNet-18\n",
    "            max_samples (int, optional): Limit dataset to first N samples for testing\n",
    "            root_dir (str): Root path for dataset (uses ROOT_DIR if None)\n",
    "        \"\"\"\n",
    "        self.meta = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Limit dataset size for testing\n",
    "        if max_samples is not None:\n",
    "            self.meta = self.meta.head(max_samples)\n",
    "            print(f\"Debug Mode: Using only {len(self.meta)} samples\")\n",
    "        \n",
    "        # Use global path constants if not provided\n",
    "        self.root_dir = Path(csv_path).parent\n",
    "        self.data_root = Path(root_dir) if root_dir else Path(ROOT_DIR)\n",
    "        \n",
    "        # Path conversion mappings (Linux <-> Windows)\n",
    "        self.path_mappings = [\n",
    "            (\"/run/media/neuronetix/BACKUP/Dataset/VOX/manual\", WINDOWS_PATHS[\"root_dir\"]),\n",
    "            (\"E:/Dataset/VOX/manual\", LINUX_PATHS[\"root_dir\"]),\n",
    "        ]\n",
    "        \n",
    "        self.clip_length = clip_length\n",
    "        self.fps = fps\n",
    "        self.target_frames = int(clip_length * fps)  # 4.0 * 25 = 100 frames\n",
    "        self.video_size = video_size\n",
    "\n",
    "    def convert_path(self, path_str):\n",
    "        \"\"\"Convert path between Linux and Windows based on current OS.\"\"\"\n",
    "        if IS_WINDOWS:\n",
    "            # Convert Linux to Windows\n",
    "            path_str = path_str.replace(LINUX_PATHS[\"root_dir\"], WINDOWS_PATHS[\"root_dir\"])\n",
    "        else:\n",
    "            # Convert Windows to Linux\n",
    "            path_str = path_str.replace(WINDOWS_PATHS[\"root_dir\"], LINUX_PATHS[\"root_dir\"])\n",
    "        return path_str\n",
    "\n",
    "    def load_video_frames(self, video_path, start_time):\n",
    "        \"\"\"\n",
    "        Load video frames and resize to target size (no face cropping).\n",
    "        Args:\n",
    "            video_path: Path to video file\n",
    "            start_time: Start time in seconds\n",
    "        Returns: Tensor [Channels, Time, H, W]\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        \n",
    "        # Get Video Properties\n",
    "        vid_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if vid_fps == 0 or np.isnan(vid_fps): \n",
    "            vid_fps = 25.0\n",
    "            \n",
    "        # Calculate Start Frame Index\n",
    "        start_frame_idx = int(start_time * vid_fps)\n",
    "        \n",
    "        # Seek to exact frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame_idx)\n",
    "        \n",
    "        frames = []\n",
    "        \n",
    "        for i in range(self.target_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resize to target size (whole frame, no cropping)\n",
    "            frame = cv2.resize(frame, self.video_size)\n",
    "            \n",
    "            frames.append(frame)\n",
    "            \n",
    "        cap.release()\n",
    "        \n",
    "        # Handle Edge Case: Video ended too early\n",
    "        if len(frames) < self.target_frames:\n",
    "            if len(frames) == 0:\n",
    "                frames = [np.zeros((self.video_size[0], self.video_size[1], 3), dtype=np.uint8)] * self.target_frames\n",
    "            else:\n",
    "                padding = [frames[-1]] * (self.target_frames - len(frames))\n",
    "                frames.extend(padding)\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        buffer = np.array(frames, dtype=np.float32) / 255.0\n",
    "        tensor = torch.from_numpy(buffer)\n",
    "        return tensor.permute(3, 0, 1, 2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row = self.meta.iloc[idx]\n",
    "        \n",
    "        # 1. Get Paths & Info\n",
    "        filename = row['filename']\n",
    "        vid_path_str = row['video_path']\n",
    "        start_time = float(row['start_time'])\n",
    "        \n",
    "        # Convert path based on current OS\n",
    "        vid_path_str = self.convert_path(vid_path_str)\n",
    "        \n",
    "        mixed_path = self.root_dir / \"mixed\" / f\"{filename}.wav\"\n",
    "        clean_path = self.root_dir / \"clean\" / f\"{filename}.wav\"\n",
    "\n",
    "        # 2. Load Audio\n",
    "        mixed_audio, _ = torchaudio.load(mixed_path)\n",
    "        clean_audio, _ = torchaudio.load(clean_path)\n",
    "\n",
    "        # 3. Load Video (whole frame, no face cropping)\n",
    "        video_tensor = self.load_video_frames(vid_path_str, start_time)\n",
    "\n",
    "        # 4. Ensure audio length matches exactly\n",
    "        target_samples = int(self.clip_length * 16000)\n",
    "        \n",
    "        if mixed_audio.shape[1] > target_samples:\n",
    "            mixed_audio = mixed_audio[:, :target_samples]\n",
    "            clean_audio = clean_audio[:, :target_samples]\n",
    "        elif mixed_audio.shape[1] < target_samples:\n",
    "            pad_size = target_samples - mixed_audio.shape[1]\n",
    "            mixed_audio = torch.nn.functional.pad(mixed_audio, (0, pad_size))\n",
    "            clean_audio = torch.nn.functional.pad(clean_audio, (0, pad_size))\n",
    "\n",
    "        return mixed_audio, clean_audio, video_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualStream(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisualStream, self).__init__()\n",
    "        # Load ResNet-18\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove classification head\n",
    "        modules = list(resnet.children())[:-1] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Project 512 -> 256 (V)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, VISUAL_DIM),\n",
    "            nn.BatchNorm1d(VISUAL_DIM),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # FIXED: ImageNet normalization for pretrained ResNet\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 3, Time, H, W] where H, W can be any size (e.g., 224x224)\n",
    "        B, C, T, H, W = x.shape\n",
    "        \n",
    "        # Fold Time into Batch\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
    "        \n",
    "        # FIXED: Apply ImageNet normalization before ResNet\n",
    "        x = (x - self.mean) / self.std\n",
    "        \n",
    "        # Extract Features (ResNet handles any input size via adaptive pooling)\n",
    "        x = self.resnet(x)       # [B*T, 512, 1, 1]\n",
    "        x = x.view(B * T, -1)    # [B*T, 512]\n",
    "        \n",
    "        # Project to 256\n",
    "        x = self.projection(x)   # [B*T, 256]\n",
    "        \n",
    "        # Unfold Time\n",
    "        x = x.view(B, T, -1).permute(0, 2, 1) # [B, 256, Time]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialStream(nn.Module):\n",
    "    def __init__(self, num_mics=4):\n",
    "        super(SpatialStream, self).__init__()\n",
    "        \n",
    "        # We compute GCC-PHAT for all pairs. \n",
    "        # For 4 mics, pairs = 4*(3)/2 = 6 pairs.\n",
    "        self.num_pairs = (num_mics * (num_mics - 1)) // 2\n",
    "        \n",
    "        # FIXED: Spatial CNN Encoder\n",
    "        # - Changed from kernel_size=1 to larger kernels (31, 15) to capture temporal patterns\n",
    "        # - Changed from BatchNorm1d to GroupNorm for stability with small batch sizes\n",
    "        # Input: [Batch, Pairs(6), Lags, Time]\n",
    "        # We treat Pairs as Channels\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.num_pairs, 64, kernel_size=31, stride=1, padding=15),\n",
    "            nn.GroupNorm(1, 64),  # Changed from BatchNorm1d\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=15, stride=1, padding=7),\n",
    "            nn.GroupNorm(1, 128),  # Changed from BatchNorm1d\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(128, SPATIAL_DIM, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def compute_gcc_phat(self, x):\n",
    "        \"\"\"\n",
    "        Compute Generalized Cross-Correlation Phase Transform (GCC-PHAT)\n",
    "        Input x: [Batch, Mics, Samples]\n",
    "        \"\"\"\n",
    "        B, M, L = x.shape\n",
    "        \n",
    "        # 1. FFT\n",
    "        # n_fft matches window size roughly\n",
    "        X = torch.fft.rfft(x, dim=-1)\n",
    "        \n",
    "        # 2. Compute Pairs\n",
    "        # We want to cross-correlate every pair (i, j)\n",
    "        pairs = []\n",
    "        for i in range(M):\n",
    "            for j in range(i + 1, M):\n",
    "                # Cross-spectrum: X_i * conj(X_j)\n",
    "                R = X[:, i, :] * torch.conj(X[:, j, :])\n",
    "                # Normalization (PHAT): Divide by magnitude\n",
    "                R = R / (torch.abs(R) + 1e-8)\n",
    "                # IFFT to get time-domain correlation\n",
    "                r = torch.fft.irfft(R, dim=-1)\n",
    "                \n",
    "                # Apply shift/lag window (we assume delays are small)\n",
    "                # This makes it a feature vector per time frame is tricky without STFT.\n",
    "                # Simplified: We treat the whole clip's correlation as a static spatial signature\n",
    "                # OR (Better): We perform this on STFT frames. \n",
    "                \n",
    "                # For simplicity in this implementation, we will use a learnable \n",
    "                # layer instead of raw GCC-PHAT if raw is too complex to batch.\n",
    "                # BUT, let's assume the input here is actually the GCC features.\n",
    "                pairs.append(r)\n",
    "                \n",
    "        return torch.stack(pairs, dim=1) # [B, 6, Samples]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 4, Samples]\n",
    "        \n",
    "        # In a real heavy model, we do STFT -> GCC-PHAT -> CNN.\n",
    "        # Here, we will use a \"Learnable Spatial Encoder\" which is faster/easier\n",
    "        # and often outperforms analytical GCC-PHAT.\n",
    "        \n",
    "        # 1. Extract correlations implicitly via 1D Conv across channels\n",
    "        # [B, 4, T] -> [B, 128, T]\n",
    "        # We pool over time to get a Global Spatial Signature S\n",
    "        \n",
    "        gcc_feat = self.compute_gcc_phat(x) # [B, 6, Samples]\n",
    "        \n",
    "        # Encode features\n",
    "        x = self.encoder(gcc_feat) # [B, 128, Samples]\n",
    "        \n",
    "        # Global Average Pooling to get single vector S \\in R^128\n",
    "        x = torch.mean(x, dim=-1)  # [B, 128]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c61e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMLayer(nn.Module):\n",
    "    def __init__(self, in_channels, cond_dim):\n",
    "        super(FiLMLayer, self).__init__()\n",
    "        # We map the Conditioning (S+V) to Gamma (Scale) and Beta (Shift)\n",
    "        self.conv_gamma = nn.Conv1d(cond_dim, in_channels, 1)\n",
    "        self.conv_beta = nn.Conv1d(cond_dim, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # x: [Batch, Channels, Time]\n",
    "        # condition: [Batch, Cond_Dim, Time]\n",
    "        \n",
    "        gamma = self.conv_gamma(condition)  # [B, C, T]\n",
    "        beta = self.conv_beta(condition)    # [B, C, T]\n",
    "            \n",
    "        # FiLM Formula: Gamma * x + Beta\n",
    "        return (gamma * x) + beta\n",
    "\n",
    "class ExtractionBlock(nn.Module):\n",
    "    \"\"\" TCN Block with FiLM Conditioning \"\"\"\n",
    "    def __init__(self, in_channels, hid_channels, cond_dim, dilation):\n",
    "        super(ExtractionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, hid_channels, 1)\n",
    "        self.norm1 = nn.GroupNorm(1, hid_channels)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        # FiLM comes after first activation usually\n",
    "        self.film = FiLMLayer(hid_channels, cond_dim)\n",
    "        \n",
    "        self.dconv = nn.Conv1d(hid_channels, hid_channels, 3, \n",
    "                               groups=hid_channels, padding=dilation, dilation=dilation)\n",
    "        self.norm2 = nn.GroupNorm(1, hid_channels)\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(hid_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        # Apply FiLM Conditioning\n",
    "        # The condition (S+V) modulates the features here\n",
    "        x = self.film(x, condition)\n",
    "        \n",
    "        x = self.dconv(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a289d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IsoNet, self).__init__()\n",
    "        \n",
    "        # 1. Streams\n",
    "        self.visual_stream = VisualStream()  # Output: 256\n",
    "        self.spatial_stream = SpatialStream(AUDIO_CHANNELS) # Output: 128\n",
    "        \n",
    "        # 2. Audio Encoder (Purple box start)\n",
    "        self.audio_enc = nn.Conv1d(AUDIO_CHANNELS, AUDIO_ENC_DIM, kernel_size=16, stride=8, bias=False)\n",
    "        \n",
    "        # 3. Conditioning Prep\n",
    "        # We concatenate S (128) + V (256) = 384\n",
    "        self.cond_dim = SPATIAL_DIM + VISUAL_DIM\n",
    "        \n",
    "        # 4. TCN with FiLM (Purple box middle)\n",
    "        self.tcn_blocks = nn.ModuleList([\n",
    "            ExtractionBlock(AUDIO_ENC_DIM, 128, self.cond_dim, dilation=2**i) \n",
    "            for i in range(8)\n",
    "        ])\n",
    "        \n",
    "        # 5. Mask Decoder (Purple box end)\n",
    "        self.mask_conv = nn.Conv1d(AUDIO_ENC_DIM, AUDIO_ENC_DIM, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # 6. Audio Decoder (Reconstructs waveform)\n",
    "        self.audio_dec = nn.ConvTranspose1d(AUDIO_ENC_DIM, 1, kernel_size=16, stride=8, bias=False)\n",
    "\n",
    "    def forward(self, audio_mix, video_frames):\n",
    "        # audio_mix: [B, 4, Samples]\n",
    "        # video_frames: [B, 3, T_v, H, W]\n",
    "        \n",
    "        # --- A. Spatial Stream ---\n",
    "        # Get global spatial embedding S\n",
    "        S = self.spatial_stream(audio_mix) # [B, 128]\n",
    "        \n",
    "        # --- B. Visual Stream ---\n",
    "        # Get visual embedding V\n",
    "        V = self.visual_stream(video_frames) # [B, 256, T_v]\n",
    "        \n",
    "        # --- C. Audio Encoding ---\n",
    "        audio_feat = self.audio_enc(audio_mix) # [B, 512, T_a]\n",
    "        \n",
    "        # --- D. Synchronization (Upsampling) ---\n",
    "        # Video (25 FPS) is slower than Audio Frames. Upsample V to match Audio T_a\n",
    "        V_upsampled = F.interpolate(V, size=audio_feat.shape[-1], mode='nearest')\n",
    "        \n",
    "        # Expand S to match time dimension: [B, 128] -> [B, 128, T_a]\n",
    "        S_expanded = S.unsqueeze(-1).expand(-1, -1, audio_feat.shape[-1])\n",
    "        \n",
    "        # Concatenate S + V to create Conditioning Vector\n",
    "        # Shape: [B, 384, T_a]\n",
    "        condition = torch.cat([S_expanded, V_upsampled], dim=1)\n",
    "        \n",
    "        # --- E. FiLM Extraction Loop ---\n",
    "        x = audio_feat\n",
    "        for block in self.tcn_blocks:\n",
    "            # We pass the condition to every block\n",
    "            x = block(x, condition)\n",
    "            \n",
    "        # --- F. Masking & Decoding ---\n",
    "        mask = self.sigmoid(self.mask_conv(x))\n",
    "        masked_feat = audio_feat * mask\n",
    "        clean_speech = self.audio_dec(masked_feat)\n",
    "        \n",
    "        return clean_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8264aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Model\n",
    "# model = IsoNet().to(DEVICE)\n",
    "# print(f\"IsoNet Created. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# # Dummy Data\n",
    "# dummy_audio = torch.randn(2, 4, 64000).to(DEVICE)     # 4 seconds audio\n",
    "# dummy_video = torch.randn(2, 3, 100, 112, 112).to(DEVICE) # 100 frames\n",
    "\n",
    "# # Forward Pass\n",
    "# output = model(dummy_audio, dummy_video)\n",
    "# print(f\"Input: {dummy_audio.shape}\")\n",
    "# print(f\"Output: {output.shape}\")\n",
    "\n",
    "# # Check\n",
    "# if output.shape[1] == 1 and abs(output.shape[-1] - 64000) < 100:\n",
    "#     print(\"IsoNet Architecture matches diagram successfully!\")\n",
    "# else:\n",
    "#     print(\"IsoNet Architecture does not match diagram.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def si_snr_loss(estimate, reference, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Scale-Invariant SNR Loss.\n",
    "    Args:\n",
    "        estimate: [Batch, Samples] - The predicted audio\n",
    "        reference: [Batch, Samples] - The clean ground truth\n",
    "    Returns:\n",
    "        Scalar Loss (Negative SI-SNR)\n",
    "    \"\"\"\n",
    "    # 1. Zero-mean the signals\n",
    "    estimate = estimate - torch.mean(estimate, dim=-1, keepdim=True)\n",
    "    reference = reference - torch.mean(reference, dim=-1, keepdim=True)\n",
    "    \n",
    "    # 2. Calculate optimal scaling factor (alpha)\n",
    "    # Dot product <ref, est> / <ref, ref>\n",
    "    ref_energy = torch.sum(reference ** 2, dim=-1, keepdim=True) + epsilon\n",
    "    dot = torch.sum(reference * estimate, dim=-1, keepdim=True)\n",
    "    alpha = dot / ref_energy\n",
    "    \n",
    "    # 3. Projection\n",
    "    target = alpha * reference\n",
    "    noise = estimate - target\n",
    "    \n",
    "    # 4. SI-SNR Calculation\n",
    "    si_snr = 10 * torch.log10(\n",
    "        torch.sum(target ** 2, dim=-1) / (torch.sum(noise ** 2, dim=-1) + epsilon)\n",
    "    )\n",
    "    \n",
    "    # 5. Return negative because we want to MAXIMIZE SNR (minimize loss)\n",
    "    return -torch.mean(si_snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ec3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only 50 samples for testing (increase once working)\n",
    "train_ds = IsoNetDataset(TRAIN_CSV, max_samples=10, video_size=(224, 224))\n",
    "val_ds = IsoNetDataset(VAL_CSV, max_samples=5, video_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsoNet().to(DEVICE)\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "# 3. Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Progress Bar\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for i, (mixed, clean, video) in enumerate(pbar):\n",
    "        try:\n",
    "            # Move to GPU\n",
    "            mixed = mixed.to(DEVICE)   # [B, 4, T]\n",
    "            clean = clean.to(DEVICE)   # [B, 1, T]\n",
    "            video = video.to(DEVICE)   # [B, 3, T, H, W]\n",
    "            \n",
    "            # Debug shapes and audio scaling on first iteration\n",
    "            if i == 0 and epoch == 0:\n",
    "                print(f\"Mixed shape: {mixed.shape}, Clean shape: {clean.shape}, Video shape: {video.shape}\")\n",
    "                print(f\"Audio Max: {mixed.abs().max():.4f}, Audio Min: {mixed.abs().min():.4f}\")\n",
    "                if mixed.abs().max() == 0.0:\n",
    "                    print(\"WARNING: Audio is silent! Check your data loading.\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            estimate = model(mixed, video) # [B, 1, T]\n",
    "            \n",
    "            # Verify output shape matches input\n",
    "            if estimate.shape[-1] != clean.shape[-1]:\n",
    "                print(f\"Shape mismatch! Estimate: {estimate.shape}, Clean: {clean.shape}\")\n",
    "                # Trim to minimum length\n",
    "                min_len = min(estimate.shape[-1], clean.shape[-1])\n",
    "                estimate = estimate[..., :min_len]\n",
    "                clean = clean[..., :min_len]\n",
    "            \n",
    "            # Calculate Loss (Squeeze channels to match [B, T])\n",
    "            loss = si_snr_loss(estimate.squeeze(1), clean.squeeze(1))\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Clipping (Prevents crashes in TCNs)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.2f}\"})\n",
    "            \n",
    "            # Clear GPU cache periodically\n",
    "            if i % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error at batch {i}: {str(e)}\")\n",
    "            print(f\"Mixed: {mixed.shape}, Clean: {clean.shape}, Video: {video.shape}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # 4. Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mixed, clean, video in val_loader:\n",
    "            try:\n",
    "                mixed, clean, video = mixed.to(DEVICE), clean.to(DEVICE), video.to(DEVICE)\n",
    "                estimate = model(mixed, video)\n",
    "                \n",
    "                # Handle shape mismatch in validation too\n",
    "                if estimate.shape[-1] != clean.shape[-1]:\n",
    "                    min_len = min(estimate.shape[-1], clean.shape[-1])\n",
    "                    estimate = estimate[..., :min_len]\n",
    "                    clean = clean[..., :min_len]\n",
    "                \n",
    "                loss = si_snr_loss(estimate.squeeze(1), clean.squeeze(1))\n",
    "                val_loss += loss.item()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Validation error: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else float('inf')\n",
    "    \n",
    "    # 5. Logging & Saving\n",
    "    print(f\"\\nEpoch {epoch+1}: Train Loss {avg_train_loss:.4f} | Val Loss {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Save Last\n",
    "    torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/last.pth\")\n",
    "    \n",
    "    # Save Best\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
    "        print(\"New Best Model Saved!\")\n",
    "    \n",
    "    # Clear cache after each epoch\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b49901",
   "metadata": {},
   "source": [
    "## Visual Encoding Test\n",
    "\n",
    "Visualize what the visual encoding model sees. This shows the full-frame input (224x224) that goes into ResNet-18, without face cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test dataset with full frame encoding (no face cropping)\n",
    "print(\"Loading dataset sample to visualize visual encoding input...\")\n",
    "test_ds = IsoNetDataset(TRAIN_CSV, max_samples=5, video_size=(224, 224))\n",
    "\n",
    "# Load a sample\n",
    "mixed, clean, video = test_ds[2]\n",
    "\n",
    "print(\"\\n--- Tensor Shapes ---\")\n",
    "print(f\"Mixed Audio: {mixed.shape}  (Expected: [4, 64000])\")\n",
    "print(f\"Clean Audio: {clean.shape}  (Expected: [1, 64000])\")\n",
    "print(f\"Video:       {video.shape}  (Expected: [3, 100, 224, 224])\")\n",
    "\n",
    "# Create visualization of what ResNet-18 sees\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Visual Encoding Input: Full Frame 224x224 (No Face Cropping)', fontsize=16)\n",
    "\n",
    "# Sample 10 frames evenly across the 4-second clip\n",
    "sample_frames = np.linspace(0, video.shape[1]-1, 10, dtype=int)\n",
    "\n",
    "for idx, frame_num in enumerate(sample_frames):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    \n",
    "    # Permute from [C, T, H, W] -> [H, W, C] for display\n",
    "    frame_tensor = video[:, frame_num, :, :].permute(1, 2, 0)\n",
    "    axes[row, col].imshow(frame_tensor.numpy())\n",
    "    axes[row, col].set_title(f'Frame {frame_num}', fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visual_encoding_input.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisual encoding input visualization complete!\")\n",
    "print(\"Note: The model sees the entire frame without face-specific cropping.\")\n",
    "print(\"Effect: Model must learn to focus on relevant regions (face/speaker) automatically.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
