{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7624bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# --- CONFIGURATION FROM DIAGRAM ---\n",
    "VISUAL_DIM = 256       # Output of Visual Stream (V)\n",
    "SPATIAL_DIM = 128      # Output of Spatial Stream (S)\n",
    "AUDIO_ENC_DIM = 512    # Internal Audio Feature Dimension\n",
    "AUDIO_CHANNELS = 4     # Number of Mics\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6852e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()        # free cached memory\n",
    "torch.cuda.synchronize()        # wait for all kernels to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualStream(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisualStream, self).__init__()\n",
    "        # Load ResNet-18\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove classification head\n",
    "        modules = list(resnet.children())[:-1] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Project 512 -> 256 (V)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, VISUAL_DIM),\n",
    "            nn.BatchNorm1d(VISUAL_DIM),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 3, Time, 112, 112]\n",
    "        B, C, T, H, W = x.shape\n",
    "        \n",
    "        # Fold Time into Batch\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
    "        \n",
    "        # Extract Features\n",
    "        x = self.resnet(x)       # [B*T, 512, 1, 1]\n",
    "        x = x.view(B * T, -1)    # [B*T, 512]\n",
    "        \n",
    "        # Project to 256\n",
    "        x = self.projection(x)   # [B*T, 256]\n",
    "        \n",
    "        # Unfold Time\n",
    "        x = x.view(B, T, -1).permute(0, 2, 1) # [B, 256, Time]\n",
    "        \n",
    "        # TODO: Temporal Average Pooling (to get a single vector per clip, optional)\n",
    "        # Or usually, we upsample this to match audio. \n",
    "        # Based on FiLM architectures, we usually keep the time dimension \n",
    "        # and upsample it later.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbbfac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialStream(nn.Module):\n",
    "    def __init__(self, num_mics=4):\n",
    "        super(SpatialStream, self).__init__()\n",
    "        \n",
    "        # We compute GCC-PHAT for all pairs. \n",
    "        # For 4 mics, pairs = 4*(3)/2 = 6 pairs.\n",
    "        self.num_pairs = (num_mics * (num_mics - 1)) // 2\n",
    "        \n",
    "        # Spatial CNN Encoder\n",
    "        # Input: [Batch, Pairs(6), Lags, Time]\n",
    "        # We treat Pairs as Channels\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.num_pairs, 64, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(128, SPATIAL_DIM, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def compute_gcc_phat(self, x):\n",
    "        \"\"\"\n",
    "        Compute Generalized Cross-Correlation Phase Transform (GCC-PHAT)\n",
    "        Input x: [Batch, Mics, Samples]\n",
    "        \"\"\"\n",
    "        B, M, L = x.shape\n",
    "        \n",
    "        # 1. FFT\n",
    "        # n_fft matches window size roughly\n",
    "        X = torch.fft.rfft(x, dim=-1)\n",
    "        \n",
    "        # 2. Compute Pairs\n",
    "        # We want to cross-correlate every pair (i, j)\n",
    "        pairs = []\n",
    "        for i in range(M):\n",
    "            for j in range(i + 1, M):\n",
    "                # Cross-spectrum: X_i * conj(X_j)\n",
    "                R = X[:, i, :] * torch.conj(X[:, j, :])\n",
    "                # Normalization (PHAT): Divide by magnitude\n",
    "                R = R / (torch.abs(R) + 1e-8)\n",
    "                # IFFT to get time-domain correlation\n",
    "                r = torch.fft.irfft(R, dim=-1)\n",
    "                \n",
    "                # Apply shift/lag window (we assume delays are small)\n",
    "                # This makes it a feature vector per time frame is tricky without STFT.\n",
    "                # Simplified: We treat the whole clip's correlation as a static spatial signature\n",
    "                # OR (Better): We perform this on STFT frames. \n",
    "                \n",
    "                # For simplicity in this implementation, we will use a learnable \n",
    "                # layer instead of raw GCC-PHAT if raw is too complex to batch.\n",
    "                # BUT, let's assume the input here is actually the GCC features.\n",
    "                pairs.append(r)\n",
    "                \n",
    "        return torch.stack(pairs, dim=1) # [B, 6, Samples]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 4, Samples]\n",
    "        \n",
    "        # In a real heavy model, we do STFT -> GCC-PHAT -> CNN.\n",
    "        # Here, we will use a \"Learnable Spatial Encoder\" which is faster/easier\n",
    "        # and often outperforms analytical GCC-PHAT.\n",
    "        \n",
    "        # 1. Extract correlations implicitly via 1D Conv across channels\n",
    "        # [B, 4, T] -> [B, 128, T]\n",
    "        # We pool over time to get a Global Spatial Signature S\n",
    "        \n",
    "        gcc_feat = self.compute_gcc_phat(x) # [B, 6, Samples]\n",
    "        \n",
    "        # Encode features\n",
    "        x = self.encoder(gcc_feat) # [B, 128, Samples]\n",
    "        \n",
    "        # Global Average Pooling to get single vector S \\in R^128\n",
    "        x = torch.mean(x, dim=-1)  # [B, 128]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c61e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMLayer(nn.Module):\n",
    "    def __init__(self, in_channels, cond_dim):\n",
    "        super(FiLMLayer, self).__init__()\n",
    "        # We map the Conditioning (S+V) to Gamma (Scale) and Beta (Shift)\n",
    "        self.conv_gamma = nn.Conv1d(cond_dim, in_channels, 1)\n",
    "        self.conv_beta = nn.Conv1d(cond_dim, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # x: [Batch, Channels, Time]\n",
    "        # condition: [Batch, Cond_Dim, Time]\n",
    "        \n",
    "        gamma = self.conv_gamma(condition)  # [B, C, T]\n",
    "        beta = self.conv_beta(condition)    # [B, C, T]\n",
    "            \n",
    "        # FiLM Formula: Gamma * x + Beta\n",
    "        return (gamma * x) + beta\n",
    "\n",
    "class ExtractionBlock(nn.Module):\n",
    "    \"\"\" TCN Block with FiLM Conditioning \"\"\"\n",
    "    def __init__(self, in_channels, hid_channels, cond_dim, dilation):\n",
    "        super(ExtractionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, hid_channels, 1)\n",
    "        self.norm1 = nn.GroupNorm(1, hid_channels)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        # FiLM comes after first activation usually\n",
    "        self.film = FiLMLayer(hid_channels, cond_dim)\n",
    "        \n",
    "        self.dconv = nn.Conv1d(hid_channels, hid_channels, 3, \n",
    "                               groups=hid_channels, padding=dilation, dilation=dilation)\n",
    "        self.norm2 = nn.GroupNorm(1, hid_channels)\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(hid_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        # Apply FiLM Conditioning\n",
    "        # The condition (S+V) modulates the features here\n",
    "        x = self.film(x, condition)\n",
    "        \n",
    "        x = self.dconv(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05a289d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IsoNet, self).__init__()\n",
    "        \n",
    "        # 1. Streams\n",
    "        self.visual_stream = VisualStream()  # Output: 256\n",
    "        self.spatial_stream = SpatialStream(AUDIO_CHANNELS) # Output: 128\n",
    "        \n",
    "        # 2. Audio Encoder (Purple box start)\n",
    "        self.audio_enc = nn.Conv1d(AUDIO_CHANNELS, AUDIO_ENC_DIM, kernel_size=16, stride=8, bias=False)\n",
    "        \n",
    "        # 3. Conditioning Prep\n",
    "        # We concatenate S (128) + V (256) = 384\n",
    "        self.cond_dim = SPATIAL_DIM + VISUAL_DIM\n",
    "        \n",
    "        # 4. TCN with FiLM (Purple box middle)\n",
    "        self.tcn_blocks = nn.ModuleList([\n",
    "            ExtractionBlock(AUDIO_ENC_DIM, 128, self.cond_dim, dilation=2**i) \n",
    "            for i in range(8)\n",
    "        ])\n",
    "        \n",
    "        # 5. Mask Decoder (Purple box end)\n",
    "        self.mask_conv = nn.Conv1d(AUDIO_ENC_DIM, AUDIO_ENC_DIM, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # 6. Audio Decoder (Reconstructs waveform)\n",
    "        self.audio_dec = nn.ConvTranspose1d(AUDIO_ENC_DIM, 1, kernel_size=16, stride=8, bias=False)\n",
    "\n",
    "    def forward(self, audio_mix, video_frames):\n",
    "        # audio_mix: [B, 4, Samples]\n",
    "        # video_frames: [B, 3, T_v, H, W]\n",
    "        \n",
    "        # --- A. Spatial Stream ---\n",
    "        # Get global spatial embedding S\n",
    "        S = self.spatial_stream(audio_mix) # [B, 128]\n",
    "        \n",
    "        # --- B. Visual Stream ---\n",
    "        # Get visual embedding V\n",
    "        V = self.visual_stream(video_frames) # [B, 256, T_v]\n",
    "        \n",
    "        # --- C. Audio Encoding ---\n",
    "        audio_feat = self.audio_enc(audio_mix) # [B, 512, T_a]\n",
    "        \n",
    "        # --- D. Synchronization (Upsampling) ---\n",
    "        # Video (25 FPS) is slower than Audio Frames. Upsample V to match Audio T_a\n",
    "        V_upsampled = F.interpolate(V, size=audio_feat.shape[-1], mode='nearest')\n",
    "        \n",
    "        # Expand S to match time dimension: [B, 128] -> [B, 128, T_a]\n",
    "        S_expanded = S.unsqueeze(-1).expand(-1, -1, audio_feat.shape[-1])\n",
    "        \n",
    "        # Concatenate S + V to create Conditioning Vector\n",
    "        # Shape: [B, 384, T_a]\n",
    "        condition = torch.cat([S_expanded, V_upsampled], dim=1)\n",
    "        \n",
    "        # --- E. FiLM Extraction Loop ---\n",
    "        x = audio_feat\n",
    "        for block in self.tcn_blocks:\n",
    "            # We pass the condition to every block\n",
    "            x = block(x, condition)\n",
    "            \n",
    "        # --- F. Masking & Decoding ---\n",
    "        mask = self.sigmoid(self.mask_conv(x))\n",
    "        masked_feat = audio_feat * mask\n",
    "        clean_speech = self.audio_dec(masked_feat)\n",
    "        \n",
    "        return clean_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f8264aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IsoNet Created. Parameters: 13,488,019\n",
      "Input: torch.Size([2, 4, 64000])\n",
      "Output: torch.Size([2, 1, 64000])\n",
      "IsoNet Architecture matches diagram successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "model = IsoNet().to(DEVICE)\n",
    "print(f\"IsoNet Created. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Dummy Data\n",
    "dummy_audio = torch.randn(2, 4, 64000).to(DEVICE)     # 4 seconds audio\n",
    "dummy_video = torch.randn(2, 3, 100, 112, 112).to(DEVICE) # 100 frames\n",
    "\n",
    "# Forward Pass\n",
    "output = model(dummy_audio, dummy_video)\n",
    "print(f\"Input: {dummy_audio.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "\n",
    "# Check\n",
    "if output.shape[1] == 1 and abs(output.shape[-1] - 64000) < 100:\n",
    "    print(\"IsoNet Architecture matches diagram successfully!\")\n",
    "else:\n",
    "    print(\"IsoNet Architecture does not match diagram.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
