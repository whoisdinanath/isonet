{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7624bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Mixed Precision: True\n",
      "Batch Size: 4 | Gradient Accumulation: 8\n",
      "Effective Batch Size: 32\n",
      "\n",
      "Model Configuration:\n",
      "  Visual Stream: True\n",
      "  Neural Beamformer: True\n",
      "  Spatial Stream: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import soundfile as sf\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress torchaudio deprecation warnings about 2.9 changes\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchaudio._backend.utils')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchaudio._backend.ffmpeg')\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "# from dataset import IsoNetDataset\n",
    "import os \n",
    "\n",
    "# --- CONFIGURATION FROM DIAGRAM ---\n",
    "VISUAL_DIM = 256       # Output of Visual Stream (V)\n",
    "SPATIAL_DIM = 128      # Output of Spatial Stream (S)\n",
    "AUDIO_ENC_DIM = 512    # Internal Audio Feature Dimension\n",
    "AUDIO_CHANNELS = 4     # Number of Mics\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Memory-Optimized Training Configuration for 8GB GPU\n",
    "BATCH_SIZE = 4          # Small batch size for 8GB GPU\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch size = 1 * 8 = 8\n",
    "EPOCHS = 100\n",
    "LR = 1e-4               # TCNs prefer lower learning rates\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Mixed Precision Training\n",
    "USE_AMP = True  # Automatic Mixed Precision (FP16) saves ~40% memory\n",
    "\n",
    "# ============================================================\n",
    "# MODEL FEATURE FLAGS (For Ablation Studies)\n",
    "# ============================================================\n",
    "# Turn these off to do audio-only training without multimodal features\n",
    "USE_VISUAL = True           # Use video/face features (visual stream)\n",
    "USE_BEAMFORMER = True       # Use neural beamformer (spatial filtering)\n",
    "USE_SPATIAL_STREAM = True   # Use GCC-PHAT spatial features\n",
    "\n",
    "# For pure audio-only baseline, set all to False:\n",
    "# USE_VISUAL = False\n",
    "# USE_BEAMFORMER = False\n",
    "# USE_SPATIAL_STREAM = False\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Mixed Precision: {USE_AMP}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE} | Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Visual Stream: {USE_VISUAL}\")\n",
    "print(f\"  Neural Beamformer: {USE_BEAMFORMER}\")\n",
    "print(f\"  Spatial Stream: {USE_SPATIAL_STREAM}\")\n",
    "if not USE_VISUAL and not USE_BEAMFORMER and not USE_SPATIAL_STREAM:\n",
    "    print(\"  -> AUDIO-ONLY MODE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec6852e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()        # free cached memory\n",
    "torch.cuda.synchronize()        # wait for all kernels to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "426b2fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Total Memory: 6.09 GB\n",
      "Available Memory: 6.09 GB\n",
      "Compute Capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MEMORY OPTIMIZATION FOR 8GB GPU\n",
    "# ============================================================\n",
    "\n",
    "# Clear cache and optimize PyTorch settings\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Enable cuDNN benchmarking for faster training (if input sizes are fixed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Enable TF32 on Ampere GPUs for faster training (A100, A6000, RTX 30/40 series)\n",
    "if hasattr(torch.backends.cuda, 'matmul'):\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "if hasattr(torch.backends.cudnn, 'allow_tf32'):\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Memory optimization\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "# Print GPU info\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Available Memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.2f} GB\")\n",
    "    print(f\"Compute Capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37930781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA PATHS CONFIGURATION\n",
      "============================================================\n",
      "OS:                   Linux\n",
      "ROOT_DIR:             /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich\n",
      "MULTICH_DIR:          /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich\n",
      "MP4_DIR:              /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/mp4\n",
      "MIXED_DIR:            /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/mixed\n",
      "CLEAN_DIR:            /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/clean\n",
      "VIDEO_DIR:            /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/video\n",
      "LOCAL_CLEAN_4CH_DIR:  /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/clean_4ch_duped\n",
      "METADATA_CSV:         /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/metadata.csv\n",
      "TRAIN_CSV:            /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/train.csv\n",
      "VAL_CSV:              /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/val.csv\n",
      "TEST_CSV:             /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/test.csv\n",
      "============================================================\n",
      "\n",
      "--- PATH VERIFICATION ---\n",
      "  MULTICH_DIR           : OK\n",
      "  MP4_DIR               : MISSING\n",
      "  MIXED_DIR             : OK\n",
      "  CLEAN_DIR             : OK\n",
      "  VIDEO_DIR             : OK\n",
      "  LOCAL_CLEAN_4CH_DIR   : OK\n",
      "\n",
      "--- METADATA INFO ---\n",
      "  Samples : 15000\n",
      "  Columns : ['filename', 'source_wav', 'source_video', 'start_time', 'mixed_audio', 'clean_audio', 'video_file', 'target_azimuth', 'target_elevation', 'target_distance', 'room_x', 'room_y', 'room_z', 'rt60', 'snr_db']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATASET PATHS CONFIGURATION (Centralized)\n",
    "# ============================================================\n",
    "# Update ONLY this cell when changing data locations\n",
    "\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OS DETECTION\n",
    "# ------------------------------------------------------------\n",
    "IS_WINDOWS = platform.system() == \"Windows\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PATH CONFIGURATION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Windows paths\n",
    "WINDOWS_PATHS = {\n",
    "    \"root_dir\": r\"C:\\Users\\bibek\\isolate-speech\",\n",
    "    \"multich_dir\": r\"C:\\Users\\bibek\\isolate-speech\\data\\multich\",\n",
    "    \"mp4_dir\": r\"C:\\Users\\bibek\\isolate-speech\\data\\mp4\",\n",
    "}\n",
    "\n",
    "# Linux / Kaggle paths\n",
    "LINUX_PATHS = {\n",
    "    \"root_dir\": \"/run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich\",\n",
    "    \"multich_dir\": \"/run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich\",\n",
    "    \"mp4_dir\": \"/run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/mp4\",\n",
    "}\n",
    "\n",
    "PATHS = WINDOWS_PATHS if IS_WINDOWS else LINUX_PATHS\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# GLOBAL PATH OBJECTS\n",
    "# ------------------------------------------------------------\n",
    "ROOT_DIR = Path(PATHS[\"root_dir\"])\n",
    "MULTICH_DIR = Path(PATHS[\"multich_dir\"])\n",
    "MP4_DIR = Path(PATHS[\"mp4_dir\"])\n",
    "\n",
    "# Subdirectories\n",
    "MIXED_DIR = MULTICH_DIR / \"mixed\"\n",
    "CLEAN_DIR = MULTICH_DIR / \"clean\"\n",
    "VIDEO_DIR = MULTICH_DIR / \"video\"\n",
    "\n",
    "# Metadata\n",
    "METADATA_CSV = MULTICH_DIR / \"metadata.csv\"\n",
    "TRAIN_CSV = MULTICH_DIR / \"train.csv\" if (MULTICH_DIR / \"train.csv\").exists() else METADATA_CSV\n",
    "VAL_CSV   = MULTICH_DIR / \"val.csv\"   if (MULTICH_DIR / \"val.csv\").exists()   else METADATA_CSV\n",
    "TEST_CSV  = MULTICH_DIR / \"test.csv\"  if (MULTICH_DIR / \"test.csv\").exists()  else METADATA_CSV\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOCAL WRITEABLE DIRECTORY (IMPORTANT FIX)\n",
    "# ------------------------------------------------------------\n",
    "# MUST be Path (not string)\n",
    "LOCAL_CLEAN_4CH_DIR = Path(\"/run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/clean_4ch_duped\")\n",
    "\n",
    "# Create directory if missing\n",
    "LOCAL_CLEAN_4CH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PRINT CONFIGURATION\n",
    "# ------------------------------------------------------------\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA PATHS CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"OS:                   {platform.system()}\")\n",
    "print(f\"ROOT_DIR:             {ROOT_DIR}\")\n",
    "print(f\"MULTICH_DIR:          {MULTICH_DIR}\")\n",
    "print(f\"MP4_DIR:              {MP4_DIR}\")\n",
    "print(f\"MIXED_DIR:            {MIXED_DIR}\")\n",
    "print(f\"CLEAN_DIR:            {CLEAN_DIR}\")\n",
    "print(f\"VIDEO_DIR:            {VIDEO_DIR}\")\n",
    "print(f\"LOCAL_CLEAN_4CH_DIR:  {LOCAL_CLEAN_4CH_DIR}\")\n",
    "print(f\"METADATA_CSV:         {METADATA_CSV}\")\n",
    "print(f\"TRAIN_CSV:            {TRAIN_CSV}\")\n",
    "print(f\"VAL_CSV:              {VAL_CSV}\")\n",
    "print(f\"TEST_CSV:             {TEST_CSV}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PATH VERIFICATION (SAFE)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n--- PATH VERIFICATION ---\")\n",
    "for name, path in [\n",
    "    (\"MULTICH_DIR\", MULTICH_DIR),\n",
    "    (\"MP4_DIR\", MP4_DIR),\n",
    "    (\"MIXED_DIR\", MIXED_DIR),\n",
    "    (\"CLEAN_DIR\", CLEAN_DIR),\n",
    "    (\"VIDEO_DIR\", VIDEO_DIR),\n",
    "    (\"LOCAL_CLEAN_4CH_DIR\", LOCAL_CLEAN_4CH_DIR),\n",
    "]:\n",
    "    status = \"OK\" if path.exists() else \"MISSING\"\n",
    "    print(f\"  {name:<22}: {status}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# METADATA CHECK\n",
    "# ------------------------------------------------------------\n",
    "if METADATA_CSV.exists():\n",
    "    meta_df = pd.read_csv(METADATA_CSV)\n",
    "    print(\"\\n--- METADATA INFO ---\")\n",
    "    print(f\"  Samples : {len(meta_df)}\")\n",
    "    print(f\"  Columns : {list(meta_df.columns)}\")\n",
    "else:\n",
    "    print(\"\\nMETADATA_CSV not found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a15143",
   "metadata": {},
   "source": [
    "## Spatial-Visual Augmentation Demo\n",
    "\n",
    "This demonstrates how we map face positions to match the spatial angles from the metadata.\n",
    "The core insight: VoxCeleb videos have **centered faces** (frontal interviews), but our audio simulation places speakers at **varying angles**. \n",
    "\n",
    "**Solution**: Shift the face position in the frame to match the spatial angle!\n",
    "- Azimuth (horizontal angle) → Shift face left/right\n",
    "- Elevation (vertical angle) → Shift face up/down\n",
    "- Distance → Scale face size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a9a17bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Size: (224, 224)\n",
      "Augmentation: Jittering (crop box shift, no padding)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# VIDEO SIZE CONFIGURATION\n",
    "# ============================================================\n",
    "# No spatial augmentation module needed - we use jittering in the dataset\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ===== VIDEO SIZE CONFIGURATION =====\n",
    "VIDEO_SIZE_SMALL = (224, 224)   # Original ResNet input\n",
    "VIDEO_SIZE_MEDIUM = (336, 336)  # 1.5x larger\n",
    "VIDEO_SIZE_LARGE = (448, 448)   # 2x larger\n",
    "\n",
    "# Choose active size\n",
    "VIDEO_SIZE = VIDEO_SIZE_SMALL  # 224x224 for ResNet-18\n",
    "\n",
    "print(f\"Video Size: {VIDEO_SIZE}\")\n",
    "print(f\"Augmentation: Jittering (crop box shift, no padding)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d9653ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from: /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/metadata.csv\n",
      "Total samples: 15000\n",
      "Columns: ['filename', 'source_wav', 'source_video', 'start_time', 'mixed_audio', 'clean_audio', 'video_file', 'target_azimuth', 'target_elevation', 'target_distance', 'room_x', 'room_y', 'room_z', 'rt60', 'snr_db']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# METADATA OVERVIEW (Optional - for exploration)\n",
    "# ============================================================\n",
    "\n",
    "print(f\"Loading metadata from: {METADATA_CSV}\")\n",
    "spatial_meta = pd.read_csv(METADATA_CSV)\n",
    "print(f\"Total samples: {len(spatial_meta)}\")\n",
    "print(f\"Columns: {list(spatial_meta.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04552eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation: Jittering (shift crop box before extract)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SKIP: Old augmentation visualization removed\n",
    "# ============================================================\n",
    "# The old SpatialVisualAugmenter (shift & pad) has been replaced\n",
    "# with proper jittering in the Dataset class.\n",
    "# No visualization needed - jittering just shifts the crop box.\n",
    "\n",
    "print(\"Augmentation: Jittering (shift crop box before extract)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ad7fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 4-channel clean audio copies...\n",
      "Source:      /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/clean\n",
      "Destination: /run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/clean_4ch_duped\n",
      "Found 15000 files in metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicating to 4-ch: 100%|██████████| 15000/15000 [00:44<00:00, 334.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Created 2733 new 4-ch files\n",
      "  Skipped 12267 existing files\n",
      "  Total: 15000/15000\n",
      "\n",
      "✓ Verification - Sample shape: torch.Size([4, 64000]) (Expected: [4, 64000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CREATE 4-CHANNEL CLEAN AUDIO IN LOCAL WORKING DIRECTORY\n",
    "# ============================================================\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"Creating 4-channel clean audio copies...\")\n",
    "print(f\"Source:      {CLEAN_DIR}\")\n",
    "print(f\"Destination: {LOCAL_CLEAN_4CH_DIR}\")\n",
    "\n",
    "# Get list of clean files from metadata\n",
    "if Path(METADATA_CSV).exists():\n",
    "    meta_df = pd.read_csv(METADATA_CSV)\n",
    "    clean_files = meta_df['filename'].tolist()\n",
    "    print(f\"Found {len(clean_files)} files in metadata\")\n",
    "    \n",
    "    # Process files\n",
    "    success_count = 0\n",
    "    skip_count = 0\n",
    "    \n",
    "    for filename in tqdm(clean_files, desc=\"Duplicating to 4-ch\"):\n",
    "        src_path = CLEAN_DIR / f\"{filename}.wav\"\n",
    "        dst_path = LOCAL_CLEAN_4CH_DIR / f\"{filename}.wav\"\n",
    "        \n",
    "        # Skip if already exists\n",
    "        if dst_path.exists():\n",
    "            skip_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Load mono clean\n",
    "        try:\n",
    "            audio, sr = torchaudio.load(src_path)  # [1, T]\n",
    "            \n",
    "            # Duplicate to 4 channels\n",
    "            audio_4ch = audio.repeat(4, 1)  # [4, T]\n",
    "            \n",
    "            # Save to local directory\n",
    "            torchaudio.save(dst_path, audio_4ch, sr)\n",
    "            success_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✓ Created {success_count} new 4-ch files\")\n",
    "    print(f\"  Skipped {skip_count} existing files\")\n",
    "    print(f\"  Total: {success_count + skip_count}/{len(clean_files)}\")\n",
    "    \n",
    "    # Verify a sample\n",
    "    if success_count > 0 or skip_count > 0:\n",
    "        sample_file = LOCAL_CLEAN_4CH_DIR / f\"{clean_files[0]}.wav\"\n",
    "        if sample_file.exists():\n",
    "            test_audio, test_sr = torchaudio.load(sample_file)\n",
    "            print(f\"\\n✓ Verification - Sample shape: {test_audio.shape} (Expected: [4, 64000])\")\n",
    "        else:\n",
    "            print(f\"\\nWarning: Sample file not found for verification\")\n",
    "else:\n",
    "    print(f\"Metadata CSV not found at: {METADATA_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010348c5",
   "metadata": {},
   "source": [
    "## Create 4-Channel Clean Audio (Local Copy)\n",
    "\n",
    "Duplicate the mono clean files from the remote dataset to 4-channel in local working directory. This avoids writing to the remote dataset and keeps everything in the project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SKIP: Old visualization cell removed\n",
    "# ============================================================\n",
    "print(\"Ready for dataset loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d726b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsoNetDataset(Dataset):\n",
    "    def __init__(self, csv_path, clip_length=4.0, fps=25, video_size=(224, 224), max_samples=None, \n",
    "                 augment=True, jitter_pct=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (str): Path to train.csv or val.csv\n",
    "            clip_length (float): Audio duration in seconds (must match simulation)\n",
    "            fps (int): Target frames per second for video (VoxCeleb is 25)\n",
    "            video_size (tuple): Target resize dimension (H, W) - 224x224 for ResNet-18\n",
    "            max_samples (int, optional): Limit dataset to first N samples for testing\n",
    "            augment (bool): Apply jittering augmentation (for training, disable for val/test)\n",
    "            jitter_pct (float): Max jitter as fraction of frame size (0.1 = 10%)\n",
    "        \"\"\"\n",
    "        self.meta = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Limit dataset size for testing\n",
    "        if max_samples is not None:\n",
    "            self.meta = self.meta.head(max_samples)\n",
    "            print(f\"Debug Mode: Using only {len(self.meta)} samples\")\n",
    "        \n",
    "        # Root dir = multich folder containing mixed/, clean/, video/ subdirs\n",
    "        self.root_dir = Path(csv_path).parent\n",
    "        \n",
    "        self.clip_length = clip_length\n",
    "        self.fps = fps\n",
    "        self.target_frames = int(clip_length * fps)  # 4.0 * 25 = 100 frames\n",
    "        self.video_size = video_size\n",
    "        \n",
    "        # Jittering augmentation (shift crop box, no padding!)\n",
    "        self.augment = augment\n",
    "        self.jitter_pct = jitter_pct\n",
    "        \n",
    "        # Check if spatial metadata is available\n",
    "        self.has_spatial_meta = all(col in self.meta.columns for col in ['target_azimuth', 'target_elevation', 'target_distance'])\n",
    "        \n",
    "        print(f\"Dataset: {len(self.meta)} samples | Root: {self.root_dir}\")\n",
    "        print(f\"  Augment: {augment} (jitter={jitter_pct*100:.0f}%)\")\n",
    "\n",
    "    def load_video_frames(self, video_path, start_time):\n",
    "        \"\"\"\n",
    "        Load video frames with optional jittering augmentation.\n",
    "        \n",
    "        Jittering = shift the crop box BEFORE extracting, not after.\n",
    "        This gives the model slightly different views without any padding.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            # Return black frames as fallback\n",
    "            frames = [np.zeros((self.video_size[0], self.video_size[1], 3), dtype=np.uint8)] * self.target_frames\n",
    "            buffer = np.array(frames, dtype=np.float32) / 255.0\n",
    "            return torch.from_numpy(buffer).permute(3, 0, 1, 2)\n",
    "        \n",
    "        # Get video properties\n",
    "        vid_fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        # Seek to start frame\n",
    "        start_frame_idx = int(start_time * vid_fps)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame_idx)\n",
    "        \n",
    "        # Calculate crop box (we crop a square from center, then resize)\n",
    "        # Jittering: randomly shift the crop box slightly\n",
    "        crop_size = min(frame_width, frame_height)\n",
    "        \n",
    "        # Default center crop\n",
    "        cx, cy = frame_width // 2, frame_height // 2\n",
    "        \n",
    "        if self.augment:\n",
    "            # Random jitter: shift center by up to jitter_pct of frame size\n",
    "            max_shift_x = int(frame_width * self.jitter_pct)\n",
    "            max_shift_y = int(frame_height * self.jitter_pct)\n",
    "            \n",
    "            jitter_x = np.random.randint(-max_shift_x, max_shift_x + 1) if max_shift_x > 0 else 0\n",
    "            jitter_y = np.random.randint(-max_shift_y, max_shift_y + 1) if max_shift_y > 0 else 0\n",
    "            \n",
    "            cx += jitter_x\n",
    "            cy += jitter_y\n",
    "        \n",
    "        # Calculate crop bounds (ensure we stay within frame)\n",
    "        half = crop_size // 2\n",
    "        x1 = max(0, min(cx - half, frame_width - crop_size))\n",
    "        y1 = max(0, min(cy - half, frame_height - crop_size))\n",
    "        x2 = x1 + crop_size\n",
    "        y2 = y1 + crop_size\n",
    "        \n",
    "        frames = []\n",
    "        for _ in range(self.target_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Crop (jittered or centered)\n",
    "            frame = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            # Resize to target size\n",
    "            frame = cv2.resize(frame, self.video_size)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Pad with last frame if video ended early\n",
    "        if len(frames) < self.target_frames:\n",
    "            if len(frames) == 0:\n",
    "                frames = [np.zeros((self.video_size[0], self.video_size[1], 3), dtype=np.uint8)] * self.target_frames\n",
    "            else:\n",
    "                frames.extend([frames[-1]] * (self.target_frames - len(frames)))\n",
    "        \n",
    "        # Convert to Tensor: [Time, H, W, C] -> [C, Time, H, W]\n",
    "        buffer = np.array(frames, dtype=np.float32) / 255.0\n",
    "        return torch.from_numpy(buffer).permute(3, 0, 1, 2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row = self.meta.iloc[idx]\n",
    "        \n",
    "        # 1. Get Paths & Info\n",
    "        filename = row['filename']\n",
    "        start_time = float(row['start_time'])\n",
    "        \n",
    "        # Build full paths\n",
    "        # Mixed & Video from MULTICH_DIR (remote)\n",
    "        # Clean from LOCAL_CLEAN_4CH_DIR (local working directory, 4-channel)\n",
    "        mixed_path = self.root_dir / \"mixed\" / f\"{filename}.wav\"\n",
    "        clean_path = LOCAL_CLEAN_4CH_DIR / f\"{filename}.wav\"  # Use local 4-ch clean\n",
    "        video_path = self.root_dir / \"video\" / f\"{filename}.mp4\"\n",
    "\n",
    "        # 2. Load Audio\n",
    "        mixed_audio, _ = torchaudio.load(mixed_path)\n",
    "        clean_audio, _ = torchaudio.load(clean_path)\n",
    "\n",
    "        # 3. Load Video (with jittering if augment=True)\n",
    "        video_tensor = self.load_video_frames(video_path, start_time)\n",
    "\n",
    "        # 4. Collect spatial metadata (for reference, not for augmentation)\n",
    "        spatial_meta = {}\n",
    "        if self.has_spatial_meta:\n",
    "            spatial_meta = {\n",
    "                'azimuth': float(row['target_azimuth']),\n",
    "                'elevation': float(row['target_elevation']),\n",
    "                'distance': float(row['target_distance']),\n",
    "                'snr_db': float(row.get('snr_db', 0.0))\n",
    "            }\n",
    "\n",
    "        # 5. Ensure audio length matches exactly (4.0s @ 16kHz = 64000 samples)\n",
    "        target_samples = int(self.clip_length * 16000)\n",
    "        \n",
    "        if mixed_audio.shape[1] > target_samples:\n",
    "            mixed_audio = mixed_audio[:, :target_samples]\n",
    "            clean_audio = clean_audio[:, :target_samples]\n",
    "        elif mixed_audio.shape[1] < target_samples:\n",
    "            pad_size = target_samples - mixed_audio.shape[1]\n",
    "            mixed_audio = torch.nn.functional.pad(mixed_audio, (0, pad_size))\n",
    "            clean_audio = torch.nn.functional.pad(clean_audio, (0, pad_size))\n",
    "\n",
    "        return mixed_audio, clean_audio, video_tensor, spatial_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualStream(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisualStream, self).__init__()\n",
    "        # Load ResNet-18\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove classification head\n",
    "        modules = list(resnet.children())[:-1] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Project 512 -> 256 (V)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, VISUAL_DIM),\n",
    "            nn.BatchNorm1d(VISUAL_DIM),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # FIXED: ImageNet normalization for pretrained ResNet\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 3, Time, H, W] where H, W can be any size (e.g., 224x224)\n",
    "        B, C, T, H, W = x.shape\n",
    "        \n",
    "        # Fold Time into Batch\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
    "        \n",
    "        # FIXED: Apply ImageNet normalization before ResNet\n",
    "        x = (x - self.mean) / self.std\n",
    "        \n",
    "        # Extract Features (ResNet handles any input size via adaptive pooling)\n",
    "        x = self.resnet(x)       # [B*T, 512, 1, 1]\n",
    "        x = x.view(B * T, -1)    # [B*T, 512]\n",
    "        \n",
    "        # Project to 256\n",
    "        x = self.projection(x)   # [B*T, 256]\n",
    "        \n",
    "        # Unfold Time\n",
    "        x = x.view(B, T, -1).permute(0, 2, 1) # [B, 256, Time]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialStream(nn.Module):\n",
    "    def __init__(self, num_mics=4):\n",
    "        super(SpatialStream, self).__init__()\n",
    "        \n",
    "        # We compute GCC-PHAT for all pairs. \n",
    "        # For 4 mics, pairs = 4*(3)/2 = 6 pairs.\n",
    "        self.num_pairs = (num_mics * (num_mics - 1)) // 2\n",
    "        \n",
    "        # FIXED: Spatial CNN Encoder\n",
    "        # - Changed from kernel_size=1 to larger kernels (31, 15) to capture temporal patterns\n",
    "        # - Changed from BatchNorm1d to GroupNorm for stability with small batch sizes\n",
    "        # Input: [Batch, Pairs(6), Lags, Time]\n",
    "        # We treat Pairs as Channels\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.num_pairs, 64, kernel_size=31, stride=1, padding=15),\n",
    "            nn.GroupNorm(1, 64),  # Changed from BatchNorm1d\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=15, stride=1, padding=7),\n",
    "            nn.GroupNorm(1, 128),  # Changed from BatchNorm1d\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(128, SPATIAL_DIM, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def compute_gcc_phat(self, x):\n",
    "        \"\"\"\n",
    "        Compute Generalized Cross-Correlation Phase Transform (GCC-PHAT)\n",
    "        Input x: [Batch, Mics, Samples]\n",
    "        \"\"\"\n",
    "        B, M, L = x.shape\n",
    "        \n",
    "        # 1. FFT\n",
    "        # n_fft matches window size roughly\n",
    "        X = torch.fft.rfft(x, dim=-1)\n",
    "        \n",
    "        # 2. Compute Pairs\n",
    "        # We want to cross-correlate every pair (i, j)\n",
    "        pairs = []\n",
    "        for i in range(M):\n",
    "            for j in range(i + 1, M):\n",
    "                # Cross-spectrum: X_i * conj(X_j)\n",
    "                R = X[:, i, :] * torch.conj(X[:, j, :])\n",
    "                # Normalization (PHAT): Divide by magnitude\n",
    "                R = R / (torch.abs(R) + 1e-8)\n",
    "                # IFFT to get time-domain correlation\n",
    "                r = torch.fft.irfft(R, dim=-1)\n",
    "                \n",
    "                # Apply shift/lag window (we assume delays are small)\n",
    "                # This makes it a feature vector per time frame is tricky without STFT.\n",
    "                # Simplified: We treat the whole clip's correlation as a static spatial signature\n",
    "                # OR (Better): We perform this on STFT frames. \n",
    "                \n",
    "                # For simplicity in this implementation, we will use a learnable \n",
    "                # layer instead of raw GCC-PHAT if raw is too complex to batch.\n",
    "                # BUT, let's assume the input here is actually the GCC features.\n",
    "                pairs.append(r)\n",
    "                \n",
    "        return torch.stack(pairs, dim=1) # [B, 6, Samples]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 4, Samples]\n",
    "        \n",
    "        # In a real heavy model, we do STFT -> GCC-PHAT -> CNN.\n",
    "        # Here, we will use a \"Learnable Spatial Encoder\" which is faster/easier\n",
    "        # and often outperforms analytical GCC-PHAT.\n",
    "        \n",
    "        # 1. Extract correlations implicitly via 1D Conv across channels\n",
    "        # [B, 4, T] -> [B, 128, T]\n",
    "        # We pool over time to get a Global Spatial Signature S\n",
    "        \n",
    "        gcc_feat = self.compute_gcc_phat(x) # [B, 6, Samples]\n",
    "        \n",
    "        # Encode features\n",
    "        x = self.encoder(gcc_feat) # [B, 128, Samples]\n",
    "        \n",
    "        # Global Average Pooling to get single vector S \\in R^128\n",
    "        x = torch.mean(x, dim=-1)  # [B, 128]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c61e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NEURAL BEAMFORMER\n",
    "# ============================================================\n",
    "# This applies spatial filtering BEFORE the audio encoder\n",
    "# to focus on the target direction indicated by visual cues\n",
    "\n",
    "class NeuralBeamformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable beamformer that combines multi-channel audio\n",
    "    based on spatial conditioning from visual stream.\n",
    "    \n",
    "    Instead of simple delay-and-sum, we learn optimal weights\n",
    "    for combining channels in the frequency domain.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_mics=4, n_fft=512, hop_length=128, conditioning_dim=256):\n",
    "        super(NeuralBeamformer, self).__init__()\n",
    "        self.num_mics = num_mics\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.num_freqs = n_fft // 2 + 1\n",
    "        \n",
    "        # Learnable beamforming weights conditioned on visual features\n",
    "        # Maps visual features to complex beamforming weights per frequency\n",
    "        self.weight_net = nn.Sequential(\n",
    "            nn.Linear(conditioning_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_mics * self.num_freqs * 2)  # Real + Imag for each freq & mic\n",
    "        )\n",
    "        \n",
    "        # Window for STFT\n",
    "        self.register_buffer('window', torch.hann_window(n_fft))\n",
    "        \n",
    "    def forward(self, audio, visual_condition):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio: [B, num_mics, samples] - Multi-channel audio\n",
    "            visual_condition: [B, conditioning_dim] - Visual feature (pooled)\n",
    "        \n",
    "        Returns:\n",
    "            beamformed: [B, 1, samples] - Spatially filtered audio\n",
    "        \"\"\"\n",
    "        B, M, L = audio.shape\n",
    "        \n",
    "        # Disable autocast for complex operations (ComplexHalf not fully supported)\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=False):\n",
    "            # Force float32 for STFT operations\n",
    "            audio = audio.float()\n",
    "            visual_condition = visual_condition.float()\n",
    "            \n",
    "            # 1. STFT on all channels: [B, M, F, T_stft] complex\n",
    "            stft_list = []\n",
    "            for m in range(M):\n",
    "                stft_m = torch.stft(\n",
    "                    audio[:, m, :], \n",
    "                    n_fft=self.n_fft, \n",
    "                    hop_length=self.hop_length,\n",
    "                    window=self.window,\n",
    "                    return_complex=True\n",
    "                )  # [B, F, T_stft]\n",
    "                stft_list.append(stft_m)\n",
    "            \n",
    "            X = torch.stack(stft_list, dim=1)  # [B, M, F, T_stft]\n",
    "            F_bins, T_stft = X.shape[2], X.shape[3]\n",
    "            \n",
    "            # 2. Compute beamforming weights from visual conditioning\n",
    "            # [B, conditioning_dim] -> [B, M * F * 2]\n",
    "            weights_flat = self.weight_net(visual_condition)\n",
    "            \n",
    "            # Reshape to [B, M, F, 2] (real, imag)\n",
    "            weights_flat = weights_flat.view(B, M, F_bins, 2)\n",
    "            \n",
    "            # Convert to complex: [B, M, F] - now in float32\n",
    "            W = torch.complex(weights_flat[..., 0], weights_flat[..., 1])\n",
    "            \n",
    "            # Normalize weights (sum to 1 per frequency for stable beamforming)\n",
    "            W = W / (torch.abs(W).sum(dim=1, keepdim=True) + 1e-8)\n",
    "            \n",
    "            # 3. Apply beamforming: weighted sum across microphones\n",
    "            # X: [B, M, F, T_stft], W: [B, M, F] -> [B, F, T_stft]\n",
    "            W = W.unsqueeze(-1)  # [B, M, F, 1]\n",
    "            Y = (X * W).sum(dim=1)  # [B, F, T_stft]\n",
    "            \n",
    "            # 4. iSTFT to get time-domain output\n",
    "            beamformed = torch.istft(\n",
    "                Y,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length,\n",
    "                window=self.window,\n",
    "                length=L\n",
    "            )  # [B, samples]\n",
    "        \n",
    "        return beamformed.unsqueeze(1)  # [B, 1, samples]\n",
    "\n",
    "\n",
    "class FiLMLayer(nn.Module):\n",
    "    def __init__(self, in_channels, cond_dim):\n",
    "        super(FiLMLayer, self).__init__()\n",
    "        # We map the Conditioning (S+V) to Gamma (Scale) and Beta (Shift)\n",
    "        self.conv_gamma = nn.Conv1d(cond_dim, in_channels, 1)\n",
    "        self.conv_beta = nn.Conv1d(cond_dim, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # x: [Batch, Channels, Time]\n",
    "        # condition: [Batch, Cond_Dim, Time]\n",
    "        \n",
    "        gamma = self.conv_gamma(condition)  # [B, C, T]\n",
    "        beta = self.conv_beta(condition)    # [B, C, T]\n",
    "            \n",
    "        # FiLM Formula: Gamma * x + Beta\n",
    "        return (gamma * x) + beta\n",
    "\n",
    "class ExtractionBlock(nn.Module):\n",
    "    \"\"\" TCN Block with FiLM Conditioning \"\"\"\n",
    "    def __init__(self, in_channels, hid_channels, cond_dim, dilation):\n",
    "        super(ExtractionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, hid_channels, 1)\n",
    "        self.norm1 = nn.GroupNorm(1, hid_channels)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        # FiLM comes after first activation usually\n",
    "        self.film = FiLMLayer(hid_channels, cond_dim)\n",
    "        \n",
    "        self.dconv = nn.Conv1d(hid_channels, hid_channels, 3, \n",
    "                               groups=hid_channels, padding=dilation, dilation=dilation)\n",
    "        self.norm2 = nn.GroupNorm(1, hid_channels)\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(hid_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        # Apply FiLM Conditioning\n",
    "        # The condition (S+V) modulates the features here\n",
    "        x = self.film(x, condition)\n",
    "        \n",
    "        x = self.dconv(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a289d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsoNet(nn.Module):\n",
    "    def __init__(self, use_checkpointing=True, use_beamformer=True, use_spatial_stream=True, use_visual=True):\n",
    "        super(IsoNet, self).__init__()\n",
    "\n",
    "        self.use_beamformer = use_beamformer\n",
    "        self.use_spatial_stream = use_spatial_stream\n",
    "        self.use_visual = use_visual\n",
    "\n",
    "        # 1. Streams\n",
    "        if use_visual:\n",
    "            self.visual_stream = VisualStream()  # Output: [B, 256, T_v]\n",
    "        else:\n",
    "            self.visual_stream = None\n",
    "\n",
    "        # Spatial stream is optional (for ablation studies)\n",
    "        if use_spatial_stream:\n",
    "            self.spatial_stream = SpatialStream(AUDIO_CHANNELS)  # Output: [B, 128]\n",
    "        else:\n",
    "            self.spatial_stream = None\n",
    "\n",
    "        # 2. NEW: Neural Beamformer (applies spatial filtering FIRST!)\n",
    "        if use_beamformer and use_visual:\n",
    "            self.beamformer = NeuralBeamformer(\n",
    "                num_mics=AUDIO_CHANNELS,\n",
    "                n_fft=512,\n",
    "                hop_length=128,\n",
    "                conditioning_dim=VISUAL_DIM  # Conditioned on visual features\n",
    "            )\n",
    "            # After beamforming: 1 channel instead of 4\n",
    "            audio_in_channels = 1\n",
    "        else:\n",
    "            self.beamformer = None\n",
    "            audio_in_channels = AUDIO_CHANNELS\n",
    "\n",
    "        # 3. Audio Encoder\n",
    "        self.audio_enc = nn.Conv1d(audio_in_channels, AUDIO_ENC_DIM, kernel_size=16, stride=8, bias=False)\n",
    "\n",
    "        # 4. Conditioning Prep\n",
    "        # Condition dim depends on whether spatial stream and visual are used\n",
    "        # With spatial + visual: S (128) + V (256) = 384\n",
    "        # With visual only: V (256)\n",
    "        # With spatial only: S (128)\n",
    "        # With neither: 0 (no conditioning)\n",
    "        self.cond_dim = 0\n",
    "        if use_spatial_stream:\n",
    "            self.cond_dim += SPATIAL_DIM\n",
    "        if use_visual:\n",
    "            self.cond_dim += VISUAL_DIM\n",
    "\n",
    "        # 5. TCN with FiLM (or without if no conditioning)\n",
    "        if self.cond_dim > 0:\n",
    "            self.tcn_blocks = nn.ModuleList([\n",
    "                ExtractionBlock(AUDIO_ENC_DIM, 128, self.cond_dim, dilation=2**i) \n",
    "                for i in range(8)\n",
    "            ])\n",
    "        else:\n",
    "            # Audio-only TCN without FiLM conditioning\n",
    "            self.tcn_blocks = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(AUDIO_ENC_DIM, 128, kernel_size=3, dilation=2**i, padding=2**i),\n",
    "                    nn.PReLU(),\n",
    "                    nn.Conv1d(128, AUDIO_ENC_DIM, kernel_size=1),\n",
    "                    nn.PReLU()\n",
    "                )\n",
    "                for i in range(8)\n",
    "            ])\n",
    "\n",
    "        # 6. Mask Decoder\n",
    "        self.mask_conv = nn.Conv1d(AUDIO_ENC_DIM, AUDIO_ENC_DIM, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # 7. Audio Decoder (Reconstructs waveform)\n",
    "        self.audio_dec = nn.ConvTranspose1d(AUDIO_ENC_DIM, 1, kernel_size=16, stride=8, bias=False)\n",
    "\n",
    "        # Gradient Checkpointing for memory savings\n",
    "        self.use_checkpointing = use_checkpointing\n",
    "\n",
    "    def forward(self, audio_mix, video_frames=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_mix: [B, 4, Samples] - 4-channel microphone array input\n",
    "            video_frames: [B, 3, T_v, H, W] - Video frames showing target speaker (optional)\n",
    "\n",
    "        Returns:\n",
    "            clean_speech: [B, 1, Samples] - Isolated speech of target speaker\n",
    "        \"\"\"\n",
    "        # --- A. Visual Stream (optional) ---\n",
    "        if self.use_visual and self.visual_stream is not None and video_frames is not None:\n",
    "            # Get visual embedding: [B, 256, T_v]\n",
    "            visual_feat = self.visual_stream(video_frames)\n",
    "            # Global average pool for beamformer conditioning: [B, 256]\n",
    "            visual_vector = torch.mean(visual_feat, dim=2)\n",
    "        else:\n",
    "            visual_feat = None\n",
    "            visual_vector = None\n",
    "\n",
    "        # --- B. Spatial Stream (optional) ---\n",
    "        if self.use_spatial_stream and self.spatial_stream is not None:\n",
    "            # Get global spatial embedding S: [B, 128]\n",
    "            S = self.spatial_stream(audio_mix)\n",
    "        else:\n",
    "            S = None\n",
    "\n",
    "        # --- C. NEURAL BEAMFORMING (optional) ---\n",
    "        if self.use_beamformer and self.beamformer is not None and visual_vector is not None:\n",
    "            # Beamform: [B, 4, samples] -> [B, 1, samples]\n",
    "            audio_beamformed = self.beamformer(audio_mix, visual_vector)\n",
    "        else:\n",
    "            # No beamforming: need to match expected input channels for audio encoder\n",
    "            if self.beamformer is not None:\n",
    "                # Beamformer was initialized, so encoder expects 1 channel\n",
    "                audio_beamformed = audio_mix.mean(dim=1, keepdim=True)\n",
    "            else:\n",
    "                # No beamformer, encoder expects 4 channels - keep as is\n",
    "                audio_beamformed = audio_mix\n",
    "\n",
    "        # --- D. Audio Encoding ---\n",
    "        audio_feat = self.audio_enc(audio_beamformed)  # [B, 512, T_a]\n",
    "\n",
    "        # --- E. Conditioning ---\n",
    "        if self.cond_dim > 0:\n",
    "            # Build conditioning based on available streams\n",
    "            conditions = []\n",
    "            \n",
    "            if self.use_visual and visual_feat is not None:\n",
    "                # Upsample visual features to match audio time dimension\n",
    "                visual_upsampled = F.interpolate(visual_feat, size=audio_feat.shape[-1], mode='nearest')\n",
    "                conditions.append(visual_upsampled)\n",
    "            \n",
    "            if self.use_spatial_stream and S is not None:\n",
    "                # Expand S to match time dimension: [B, 128] -> [B, 128, T_a]\n",
    "                S_expanded = S.unsqueeze(-1).expand(-1, -1, audio_feat.shape[-1])\n",
    "                conditions.append(S_expanded)\n",
    "            \n",
    "            # Concatenate all conditioning signals\n",
    "            condition = torch.cat(conditions, dim=1) if len(conditions) > 0 else None\n",
    "        else:\n",
    "            condition = None\n",
    "\n",
    "        # --- F. TCN Processing ---\n",
    "        x = audio_feat\n",
    "\n",
    "        if self.cond_dim > 0:\n",
    "            # FiLM-conditioned TCN\n",
    "            if self.use_checkpointing and self.training:\n",
    "                for block in self.tcn_blocks:\n",
    "                    x = torch.utils.checkpoint.checkpoint(block, x, condition, use_reentrant=False)\n",
    "            else:\n",
    "                for block in self.tcn_blocks:\n",
    "                    x = block(x, condition)\n",
    "        else:\n",
    "            # Audio-only TCN (no conditioning)\n",
    "            for block in self.tcn_blocks:\n",
    "                residual = x\n",
    "                x = block(x)\n",
    "                x = x + residual  # Residual connection\n",
    "            \n",
    "        # --- G. Masking & Decoding ---\n",
    "        mask = self.sigmoid(self.mask_conv(x))\n",
    "        masked_feat = audio_feat * mask\n",
    "        clean_speech = self.audio_dec(masked_feat)\n",
    "\n",
    "        return clean_speech\n",
    "\n",
    "# Print architecture summary\n",
    "print(\"IsoNet Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Visual Stream: Video -> ResNet-18 -> V [B, 256, T_v] (optional)\")\n",
    "print(\"2. Spatial Stream: 4ch Audio -> GCC-PHAT -> S [B, 128] (optional)\")\n",
    "print(\"3. Neural Beamformer: 4ch Audio + V -> Beamformed [B, 1, L] (optional)\")\n",
    "print(\"4. Audio Encoder: Beamformed -> Features [B, 512, T_a]\")\n",
    "print(\"5. FiLM TCN: Features + (S,V) -> Refined [B, 512, T_a]\")\n",
    "print(\"   (Audio-only TCN if no conditioning)\")\n",
    "print(\"6. Mask & Decode: -> Clean Speech [B, 1, L]\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8264aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Model\n",
    "# model = IsoNet().to(DEVICE)\n",
    "# print(f\"IsoNet Created. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# # Dummy Data\n",
    "# dummy_audio = torch.randn(2, 4, 64000).to(DEVICE)     # 4 seconds audio\n",
    "# dummy_video = torch.randn(2, 3, 100, 112, 112).to(DEVICE) # 100 frames\n",
    "\n",
    "# # Forward Pass\n",
    "# output = model(dummy_audio, dummy_video)\n",
    "# print(f\"Input: {dummy_audio.shape}\")\n",
    "# print(f\"Output: {output.shape}\")\n",
    "\n",
    "# # Check\n",
    "# if output.shape[1] == 1 and abs(output.shape[-1] - 64000) < 100:\n",
    "#     print(\"IsoNet Architecture matches diagram successfully!\")\n",
    "# else:\n",
    "#     print(\"IsoNet Architecture does not match diagram.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def si_snr_loss(estimate, reference, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Scale-Invariant SNR Loss with numerical stability.\n",
    "    Args:\n",
    "        estimate: [Batch, Samples] - The predicted audio (mono)\n",
    "        reference: [Batch, Channels, Samples] or [Batch, Samples] - The clean ground truth\n",
    "    Returns:\n",
    "        Scalar Loss (Negative SI-SNR), Actual SI-SNR value (for logging)\n",
    "\n",
    "    Note: We return -SI-SNR as loss to minimize. Higher SI-SNR is better.\n",
    "    \"\"\"\n",
    "    # Handle multi-channel reference by averaging to mono\n",
    "    if reference.dim() == 3:  # [B, C, T]\n",
    "        reference = reference.mean(dim=1)  # [B, T]\n",
    "\n",
    "    # 1. Zero-mean the signals\n",
    "    estimate = estimate - torch.mean(estimate, dim=-1, keepdim=True)\n",
    "    reference = reference - torch.mean(reference, dim=-1, keepdim=True)\n",
    "\n",
    "    # 2. Calculate optimal scaling factor (alpha)\n",
    "    # Dot product <ref, est> / <ref, ref>\n",
    "    ref_energy = torch.sum(reference ** 2, dim=-1, keepdim=True) + epsilon\n",
    "    dot = torch.sum(reference * estimate, dim=-1, keepdim=True)\n",
    "    alpha = dot / ref_energy\n",
    "\n",
    "    # 3. Projection\n",
    "    target = alpha * reference\n",
    "    noise = estimate - target\n",
    "\n",
    "    # 4. SI-SNR Calculation with numerical stability\n",
    "    target_energy = torch.sum(target ** 2, dim=-1) + epsilon\n",
    "    noise_energy = torch.sum(noise ** 2, dim=-1) + epsilon\n",
    "\n",
    "    si_snr = 10 * torch.log10(target_energy / noise_energy)\n",
    "\n",
    "    # Clamp to prevent extreme values\n",
    "    si_snr = torch.clamp(si_snr, min=-30, max=30)\n",
    "\n",
    "    si_snr_mean = torch.mean(si_snr)\n",
    "\n",
    "    # 5. Return negative loss and actual SI-SNR for logging\n",
    "    return -si_snr_mean, si_snr_mean.item()\n",
    "\n",
    "\n",
    "# Multi-resolution STFT loss helps preserve spectral detail\n",
    "STFT_CONFIGS = [\n",
    "    (512, 128, 512),\n",
    "    (1024, 256, 1024),\n",
    "    (2048, 512, 2048),\n",
    "]\n",
    "\n",
    "SI_SNR_WEIGHT = 1.0\n",
    "STFT_WEIGHT = 0.1  # Reduced to prevent STFT from dominating when SI-SNR is good\n",
    "\n",
    "def multi_resolution_stft_loss(estimate, reference, configs=STFT_CONFIGS, eps=1e-7):\n",
    "    \"\"\"Magnitude + log-magnitude multi-resolution STFT loss.\n",
    "    \n",
    "    Note: STFT is computed in float32 to avoid ComplexHalf warnings with AMP.\n",
    "    \"\"\"\n",
    "    if reference.dim() == 3:\n",
    "        reference = reference.mean(dim=1)\n",
    "    if estimate.dim() == 3:\n",
    "        estimate = estimate.mean(dim=1)\n",
    "\n",
    "    # Force float32 for STFT to avoid ComplexHalf experimental warning\n",
    "    estimate = estimate.float()\n",
    "    reference = reference.float()\n",
    "\n",
    "    losses = []\n",
    "    for n_fft, hop, win in configs:\n",
    "        window = torch.hann_window(win, device=estimate.device, dtype=torch.float32)\n",
    "        est_spec = torch.stft(\n",
    "            estimate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop,\n",
    "            win_length=win,\n",
    "            window=window,\n",
    "            return_complex=True,\n",
    "            normalized=False,\n",
    "        )\n",
    "        ref_spec = torch.stft(\n",
    "            reference,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop,\n",
    "            win_length=win,\n",
    "            window=window,\n",
    "            return_complex=True,\n",
    "            normalized=False,\n",
    "        )\n",
    "\n",
    "        mag_est = est_spec.abs()\n",
    "        mag_ref = ref_spec.abs()\n",
    "\n",
    "        sc_loss = torch.norm(mag_ref - mag_est, p=\"fro\") / (torch.norm(mag_ref, p=\"fro\") + eps)\n",
    "        mag_loss = torch.mean(torch.abs(torch.log(mag_ref + eps) - torch.log(mag_est + eps)))\n",
    "        losses.append(sc_loss + mag_loss)\n",
    "\n",
    "    return torch.mean(torch.stack(losses))\n",
    "\n",
    "\n",
    "def separation_loss(estimate, reference):\n",
    "    \"\"\"\n",
    "    Composite loss: SI-SNR for temporal fidelity + STFT for spectral detail.\n",
    "    \n",
    "    Returns:\n",
    "        total_loss: Combined loss to minimize (always positive, lower is better)\n",
    "        si_snr_metric: Actual SI-SNR in dB (higher is better)\n",
    "        stft_val: STFT loss component (lower is better)\n",
    "    \n",
    "    Note: total_loss = -SI_SNR + STFT, so lower total is better\n",
    "    \"\"\"\n",
    "    si_snr_loss_val, si_snr_metric = si_snr_loss(estimate, reference)\n",
    "    stft_val = multi_resolution_stft_loss(estimate, reference)\n",
    "    \n",
    "    # si_snr_loss_val is already negative SI-SNR, so adding them makes sense\n",
    "    # When SI-SNR is high (good), si_snr_loss_val is very negative\n",
    "    # When STFT is low (good), stft_val is low positive\n",
    "    # Total should trend negative for good performance, but we want positive loss\n",
    "    # Solution: Return absolute value or add offset\n",
    "    total = SI_SNR_WEIGHT * si_snr_loss_val + STFT_WEIGHT * stft_val\n",
    "    \n",
    "    return total, si_snr_metric, stft_val.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ec3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using VIDEO_SIZE: {VIDEO_SIZE}\")\n",
    "\n",
    "# Set max_samples=None for full training, or a number for quick debugging\n",
    "MAX_TRAIN_SAMPLES = None  # Use full dataset (set to e.g. 100 for quick debug)\n",
    "MAX_VAL_SAMPLES = None    # Use full dataset (set to e.g. 10 for quick debug)\n",
    "\n",
    "train_ds = IsoNetDataset(\n",
    "    TRAIN_CSV, \n",
    "    max_samples=MAX_TRAIN_SAMPLES,\n",
    "    video_size=VIDEO_SIZE,\n",
    "    augment=True,      # Enable jittering for training\n",
    "    jitter_pct=0.1     # 10% max shift\n",
    ")\n",
    "\n",
    "val_ds = IsoNetDataset(\n",
    "    VAL_CSV, \n",
    "    max_samples=MAX_VAL_SAMPLES,\n",
    "    video_size=VIDEO_SIZE,\n",
    "    augment=False,     # No augmentation for validation (deterministic)\n",
    "    jitter_pct=0.0\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(train_ds)} | Val: {len(val_ds)} | Size: {VIDEO_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-Optimized DataLoaders\n",
    "# On Windows, multiprocessing can cause issues in notebooks - use num_workers=0\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for Windows notebook compatibility\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Set to 0 for Windows notebook compatibility\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created (num_workers=0 for notebook compatibility)\")\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsoNet(\n",
    "    use_checkpointing=True,\n",
    "    use_beamformer=USE_BEAMFORMER,\n",
    "    use_spatial_stream=USE_SPATIAL_STREAM,\n",
    "    use_visual=USE_VISUAL\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "# Print active features\n",
    "active_features = []\n",
    "if USE_VISUAL:\n",
    "    active_features.append(\"Visual\")\n",
    "if USE_BEAMFORMER:\n",
    "    active_features.append(\"Beamformer\")\n",
    "if USE_SPATIAL_STREAM:\n",
    "    active_features.append(\"Spatial\")\n",
    "if not active_features:\n",
    "    active_features.append(\"Audio-Only\")\n",
    "print(f\"Active Features: {', '.join(active_features)}\")\n",
    "# print(f\"Model Size: ~{total_params * 4 / 1e6:.1f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP (Clean Logging with Early Stopping)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_si_snr = -float('inf')\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "# Early Stopping Configuration\n",
    "EARLY_STOPPING_PATIENCE = 5  # Stop if no improvement for N epochs\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Print config once at start\n",
    "print(f\"Training | {EPOCHS} epochs | BS={BATCH_SIZE}x{GRADIENT_ACCUMULATION_STEPS} | LR={LR} | AMP={USE_AMP}\")\n",
    "print(f\"Data     | Train: {len(train_ds)} | Val: {len(val_ds)} | Jitter: {train_ds.augment}\")\n",
    "print(f\"Loss Weights | SI-SNR: {SI_SNR_WEIGHT} | STFT: {STFT_WEIGHT}\")\n",
    "print(f\"Early Stopping | Patience: {EARLY_STOPPING_PATIENCE} epochs\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Epoch':<8} {'Train Loss':<12} {'Train STFT':<12} {'Train SI-SNR':<14} {'Val Loss':<12} {'Val STFT':<10} {'Val SI-SNR':<12} {'Time':<8} {'Status'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_si_snr_sum = 0\n",
    "    train_stft_sum = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    # Mask statistics\n",
    "    mask_means = []\n",
    "    mask_stds = []\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        try:\n",
    "            if len(batch_data) == 4:\n",
    "                mixed, clean, video, spatial_meta = batch_data\n",
    "            else:\n",
    "                mixed, clean, video = batch_data\n",
    "\n",
    "            mixed = mixed.to(DEVICE, non_blocking=True)\n",
    "            clean = clean.to(DEVICE, non_blocking=True)\n",
    "            video = video.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                estimate = model(mixed, video)\n",
    "\n",
    "                # Get mask for logging (before final decoding)\n",
    "                with torch.no_grad():\n",
    "                    # Access the mask from the model's last forward pass\n",
    "                    # We'll compute it again just for stats (lightweight)\n",
    "                    if model.visual_stream is not None:\n",
    "                        V = model.visual_stream(video)\n",
    "                        V_pooled = V.mean(dim=-1)\n",
    "                    else:\n",
    "                        V = None\n",
    "                        V_pooled = None\n",
    "\n",
    "                    if model.use_spatial_stream and model.spatial_stream is not None:\n",
    "                        S = model.spatial_stream(mixed)\n",
    "                    else:\n",
    "                        S = None\n",
    "\n",
    "                    if model.use_beamformer and model.beamformer is not None and V_pooled is not None:\n",
    "                        audio_beamformed = model.beamformer(mixed, V_pooled)\n",
    "                    else:\n",
    "                        # Handle case where beamformer expects 1 channel but we need to average\n",
    "                        if model.beamformer is not None:\n",
    "                            audio_beamformed = mixed.mean(dim=1, keepdim=True)\n",
    "                        else:\n",
    "                            audio_beamformed = mixed\n",
    "\n",
    "                    audio_feat = model.audio_enc(audio_beamformed)\n",
    "                    \n",
    "                    # Build condition based on available streams\n",
    "                    conditions = []\n",
    "                    if V is not None:\n",
    "                        V_upsampled = F.interpolate(V, size=audio_feat.shape[-1], mode='nearest')\n",
    "                        conditions.append(V_upsampled)\n",
    "                    \n",
    "                    if model.use_spatial_stream and S is not None:\n",
    "                        S_expanded = S.unsqueeze(-1).expand(-1, -1, audio_feat.shape[-1])\n",
    "                        conditions.append(S_expanded)\n",
    "                    \n",
    "                    condition = torch.cat(conditions, dim=1) if len(conditions) > 0 else None\n",
    "\n",
    "                    x = audio_feat\n",
    "                    if condition is not None and model.cond_dim > 0:\n",
    "                        for block in model.tcn_blocks:\n",
    "                            x = block(x, condition)\n",
    "                    else:\n",
    "                        # Audio-only TCN without conditioning\n",
    "                        for block in model.tcn_blocks:\n",
    "                            residual = x\n",
    "                            x = block(x)\n",
    "                            x = x + residual\n",
    "\n",
    "                    mask = torch.sigmoid(model.mask_conv(x))\n",
    "\n",
    "                    # Log mask statistics (every 10 batches to reduce overhead)\n",
    "                    if i % 10 == 0:\n",
    "                        mask_means.append(mask.mean().item())\n",
    "                        mask_stds.append(mask.std().item())\n",
    "\n",
    "                if estimate.shape[-1] != clean.shape[-1]:\n",
    "                    min_len = min(estimate.shape[-1], clean.shape[-1])\n",
    "                    estimate = estimate[..., :min_len]\n",
    "                    clean = clean[..., :min_len]\n",
    "\n",
    "                total_loss, si_snr_val, stft_val = separation_loss(estimate.squeeze(1), clean)\n",
    "                loss = total_loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "            train_si_snr_sum += si_snr_val\n",
    "            train_stft_sum += stft_val.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            # Progress bar shows current SI-SNR\n",
    "            current_si_snr = train_si_snr_sum / batch_count\n",
    "            current_stft = train_stft_sum / batch_count\n",
    "            pbar.set_postfix_str(f\"SI-SNR={current_si_snr:.2f}dB | STFT={current_stft:.3f} | mask={mask_means[-1] if mask_means else 0:.3f}\")\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[!] Batch {i} error: {str(e)[:50]}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "    avg_train_loss = train_loss / max(batch_count, 1)\n",
    "    avg_train_si_snr = train_si_snr_sum / max(batch_count, 1)\n",
    "    avg_train_stft = train_stft_sum / max(batch_count, 1)\n",
    "\n",
    "    # Compute mask statistics\n",
    "    avg_mask_mean = np.mean(mask_means) if mask_means else 0.0\n",
    "    avg_mask_std = np.mean(mask_stds) if mask_stds else 0.0\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss_sum = 0\n",
    "    val_si_snr_sum = 0\n",
    "    val_stft_sum = 0\n",
    "    val_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm.tqdm(val_loader, desc=\"Val\", leave=False):\n",
    "            try:\n",
    "                if len(batch_data) == 4:\n",
    "                    mixed, clean, video, _ = batch_data\n",
    "                else:\n",
    "                    mixed, clean, video = batch_data\n",
    "\n",
    "                mixed = mixed.to(DEVICE, non_blocking=True)\n",
    "                clean = clean.to(DEVICE, non_blocking=True)\n",
    "                video = video.to(DEVICE, non_blocking=True)\n",
    "\n",
    "                with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                    estimate = model(mixed, video)\n",
    "                    if estimate.shape[-1] != clean.shape[-1]:\n",
    "                        min_len = min(estimate.shape[-1], clean.shape[-1])\n",
    "                        estimate = estimate[..., :min_len]\n",
    "                        clean = clean[..., :min_len]\n",
    "                    val_total, si_snr_val, stft_val = separation_loss(estimate.squeeze(1), clean)\n",
    "                    val_loss_sum += val_total.item()\n",
    "                    val_si_snr_sum += si_snr_val\n",
    "                    val_stft_sum += stft_val.item()\n",
    "                    val_count += 1\n",
    "            except RuntimeError:\n",
    "                continue\n",
    "\n",
    "    avg_val_loss = val_loss_sum / max(val_count, 1)\n",
    "    avg_val_si_snr = val_si_snr_sum / max(val_count, 1)\n",
    "    avg_val_stft = val_stft_sum / max(val_count, 1)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    # Check for improvement (use SI-SNR as primary metric since loss can be misleading)\n",
    "    improved = avg_val_si_snr > best_val_si_snr\n",
    "\n",
    "    if improved:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_val_si_snr = avg_val_si_snr\n",
    "        early_stopping_counter = 0\n",
    "        status = \"* BEST\"\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        status = f\"  ({early_stopping_counter}/{EARLY_STOPPING_PATIENCE})\"\n",
    "\n",
    "    # Print epoch summary with SI-SNR and mask statistics\n",
    "    print(f\"[{epoch+1:02d}/{EPOCHS}]  {avg_train_loss:>10.4f}  {avg_train_stft:>10.3f}  {avg_train_si_snr:>12.2f}dB  {avg_val_loss:>10.4f}  {avg_val_stft:>10.3f}  {avg_val_si_snr:>12.2f}dB  {epoch_time/60:>6.1f}m  mask:{avg_mask_mean:.3f}±{avg_mask_std:.3f}  {status}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'train_si_snr': avg_train_si_snr,\n",
    "        'val_si_snr': avg_val_si_snr,\n",
    "        'train_stft': avg_train_stft,\n",
    "        'val_stft': avg_val_stft,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_val_si_snr': best_val_si_snr,\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "    }, f\"{CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    # Save best model\n",
    "    if improved:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': best_val_loss,\n",
    "            'train_si_snr': avg_train_si_snr,\n",
    "            'val_si_snr': avg_val_si_snr,\n",
    "            'train_stft': avg_train_stft,\n",
    "            'val_stft': avg_val_stft,\n",
    "        }, f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
    "        print(f\"         -> Saved best model to {CHECKPOINT_DIR}/best_model.pth\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Early stopping triggered! No improvement for {EARLY_STOPPING_PATIENCE} epochs.\")\n",
    "        break\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"  Best Val Loss:   {best_val_loss:.4f}\")\n",
    "print(f\"  Best Val SI-SNR: {best_val_si_snr:.2f} dB\")\n",
    "print(f\"  Saved: {CHECKPOINT_DIR}/best_model.pth\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b49901",
   "metadata": {},
   "source": [
    "## Visual Encoding Test\n",
    "\n",
    "Visualize what the visual encoding model sees. Using larger input size (336x336) instead of 224x224 to accommodate spatial augmentation - this gives more room for face shifting without cutting off content.\n",
    "\n",
    "ResNet-18 handles any input size via adaptive pooling, so the output dimension stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test dataset with jittering\n",
    "print(f\"Loading dataset sample to visualize visual encoding input...\")\n",
    "print(f\"Using VIDEO_SIZE: {VIDEO_SIZE}\")\n",
    "test_ds = IsoNetDataset(TRAIN_CSV, max_samples=5, video_size=VIDEO_SIZE, augment=True)\n",
    "\n",
    "# Load a sample\n",
    "mixed, clean, video, spatial_meta = test_ds[2]\n",
    "\n",
    "print(\"\\n--- Tensor Shapes ---\")\n",
    "print(f\"Mixed Audio: {mixed.shape}  (Expected: [4, 64000])\")\n",
    "print(f\"Clean Audio: {clean.shape}  (Expected: [1, 64000])\")\n",
    "print(f\"Video:       {video.shape}  (Expected: [3, 100, {VIDEO_SIZE[0]}, {VIDEO_SIZE[1]}])\")\n",
    "\n",
    "# Show spatial metadata\n",
    "print(f\"\\n--- Spatial Metadata ---\")\n",
    "if spatial_meta:\n",
    "    for key, val in spatial_meta.items():\n",
    "        if key in ['azimuth', 'elevation']:\n",
    "            print(f\"  {key}: {val:.4f} rad ({np.degrees(val):.1f} deg)\")\n",
    "        else:\n",
    "            print(f\"  {key}: {val:.4f}\")\n",
    "else:\n",
    "    print(\"  No spatial metadata available\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle(f'Visual Encoding Input (Jittered) - {VIDEO_SIZE[0]}x{VIDEO_SIZE[1]}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Sample 10 frames evenly across the 4-second clip\n",
    "sample_frames = np.linspace(0, video.shape[1]-1, 10, dtype=int)\n",
    "\n",
    "for idx, frame_num in enumerate(sample_frames):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    \n",
    "    # Permute from [C, T, H, W] -> [H, W, C] for display\n",
    "    frame_tensor = video[:, frame_num, :, :].permute(1, 2, 0)\n",
    "    axes[row, col].imshow(frame_tensor.numpy())\n",
    "    axes[row, col].set_title(f'Frame {frame_num}', fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visual_encoding_input.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisual encoding input visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b005f8",
   "metadata": {},
   "source": [
    "## Comprehensive Dataset Sample Check\n",
    "\n",
    "Load a random sample and visualize EVERYTHING to verify data integrity: video frames, audio waveforms, spectrograms, and metadata alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import random\n",
    "\n",
    "# Load dataset with jittering\n",
    "print(f\"Loading dataset for comprehensive check...\")\n",
    "print(f\"Using VIDEO_SIZE: {VIDEO_SIZE}\")\n",
    "check_ds = IsoNetDataset(TRAIN_CSV, max_samples=20, video_size=VIDEO_SIZE, augment=True)\n",
    "\n",
    "# Pick a random sample\n",
    "random_idx = random.randint(0, len(check_ds) - 1)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RANDOM SAMPLE CHECK - Index: {random_idx}/{len(check_ds)-1}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Load sample\n",
    "mixed_audio, clean_audio, video_tensor, spatial_meta = check_ds[random_idx]\n",
    "\n",
    "# Get metadata\n",
    "row = check_ds.meta.iloc[random_idx]\n",
    "print(f\"\\n--- METADATA ---\")\n",
    "print(f\"Filename: {row['filename']}\")\n",
    "print(f\"Video Path: {row.get('video_path', row.get('source_video', 'N/A'))}\")\n",
    "print(f\"Start Time: {row['start_time']:.2f}s\")\n",
    "\n",
    "# Show spatial metadata\n",
    "print(f\"\\n--- SPATIAL METADATA ---\")\n",
    "if spatial_meta:\n",
    "    for key, val in spatial_meta.items():\n",
    "        if key in ['azimuth', 'elevation']:\n",
    "            print(f\"  {key}: {val:.4f} rad ({np.degrees(val):.1f} deg)\")\n",
    "        else:\n",
    "            print(f\"  {key}: {val:.4f}\")\n",
    "else:\n",
    "    print(\"  No spatial metadata available\")\n",
    "\n",
    "print(f\"\\n--- TENSOR SHAPES ---\")\n",
    "print(f\"Mixed Audio: {mixed_audio.shape}  (Expected: [4, 64000])\")\n",
    "print(f\"Clean Audio: {clean_audio.shape}  (Expected: [1, 64000])\")\n",
    "print(f\"Video:       {video_tensor.shape}  (Expected: [3, 100, {VIDEO_SIZE[0]}, {VIDEO_SIZE[1]}])\")\n",
    "\n",
    "print(f\"\\n--- AUDIO STATISTICS ---\")\n",
    "print(f\"Mixed Audio - Max: {mixed_audio.max():.4f}, Min: {mixed_audio.min():.4f}, Mean: {mixed_audio.mean():.4f}\")\n",
    "print(f\"Clean Audio - Max: {clean_audio.max():.4f}, Min: {clean_audio.min():.4f}, Mean: {clean_audio.mean():.4f}\")\n",
    "\n",
    "print(f\"\\n--- VIDEO STATISTICS ---\")\n",
    "print(f\"Video - Max: {video_tensor.max():.4f}, Min: {video_tensor.min():.4f}, Mean: {video_tensor.mean():.4f}\")\n",
    "print(f\"Video - Frames: {video_tensor.shape[1]}, Duration: {video_tensor.shape[1]/25:.2f}s @ 25fps\")\n",
    "\n",
    "# Check for silent audio\n",
    "if mixed_audio.abs().max() < 1e-6:\n",
    "    print(f\"\\nWARNING: Mixed audio appears to be SILENT!\")\n",
    "if clean_audio.abs().max() < 1e-6:\n",
    "    print(f\"\\nWARNING: Clean audio appears to be SILENT!\")\n",
    "\n",
    "# Check duration mismatch\n",
    "audio_duration = mixed_audio.shape[1] / 16000\n",
    "video_duration = video_tensor.shape[1] / 25\n",
    "print(f\"\\n--- DURATION CHECK ---\")\n",
    "print(f\"Audio Duration: {audio_duration:.3f}s\")\n",
    "print(f\"Video Duration: {video_duration:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dbc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio playback - Mixed Audio (Mic 1)\n",
    "print(\"\\n🔊 Playing Mixed Audio (Microphone 1)...\")\n",
    "ipd.display(ipd.Audio(mixed_audio[0].numpy(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c153f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio playback - Clean Audio\n",
    "print(\"\\n🔊 Playing Clean Audio (Target Speech)...\")\n",
    "ipd.display(ipd.Audio(clean_audio[0].numpy(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15193ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all 4 microphone channels side-by-side\n",
    "print(\"\\n--- Multi-Channel Comparison ---\")\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 8), sharex=True, sharey=True)\n",
    "time_axis = np.arange(mixed_audio.shape[1]) / 16000\n",
    "\n",
    "for ch in range(4):\n",
    "    axes[ch].plot(time_axis, mixed_audio[ch].numpy(), linewidth=0.5)\n",
    "    axes[ch].set_ylabel(f'Mic {ch+1}')\n",
    "    axes[ch].grid(True, alpha=0.3)\n",
    "    axes[ch].set_xlim([0, time_axis[-1]])\n",
    "    \n",
    "    # Calculate RMS energy\n",
    "    rms = np.sqrt(np.mean(mixed_audio[ch].numpy()**2))\n",
    "    axes[ch].text(0.02, 0.95, f'RMS: {rms:.4f}', transform=axes[ch].transAxes, \n",
    "                  fontsize=9, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "fig.suptitle('4-Channel Microphone Array - Waveform Comparison', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"multichannel_comparison_{random_idx}.png\", dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ All visualizations complete!\")\n",
    "print(f\"✓ Files saved: dataset_check_sample_{random_idx}.png, multichannel_comparison_{random_idx}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de120568",
   "metadata": {},
   "source": [
    "## Model Inference Test\n",
    "\n",
    "Load the best trained model and test it on a validation sample. Compare input mixed audio, model output, and ground truth clean audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Load Best Model\n",
    "print(\"Loading best model checkpoint...\")\n",
    "best_model_path = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
    "\n",
    "if not os.path.exists(best_model_path):\n",
    "    print(f\"Best model not found at: {best_model_path}\")\n",
    "    print(f\"Available checkpoints:\")\n",
    "    for f in os.listdir(CHECKPOINT_DIR):\n",
    "        print(f\"  - {f}\")\n",
    "else:\n",
    "    # Initialize model\n",
    "    test_model = IsoNet(use_checkpointing=False).to(DEVICE)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Validation Loss: {checkpoint['val_loss']:.6f}\")\n",
    "    print(f\"  Training Loss: {checkpoint['train_loss']:.6f}\")\n",
    "    if 'spatial_augmentation' in checkpoint:\n",
    "        print(f\"  Spatial Augmentation: {checkpoint['spatial_augmentation']}\")\n",
    "    \n",
    "    # Load a test sample from validation set\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING INFERENCE ON VALIDATION SAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_idx = 0  # Use first validation sample\n",
    "    sample_data = val_ds[test_idx]\n",
    "    \n",
    "    # Handle both old and new format\n",
    "    if len(sample_data) == 4:\n",
    "        mixed_audio, clean_audio, video_tensor, spatial_meta = sample_data\n",
    "    else:\n",
    "        mixed_audio, clean_audio, video_tensor = sample_data\n",
    "        spatial_meta = None\n",
    "    \n",
    "    # Get metadata\n",
    "    row = val_ds.meta.iloc[test_idx]\n",
    "    print(f\"\\nTest Sample Info:\")\n",
    "    print(f\"  Filename: {row['filename']}\")\n",
    "    print(f\"  Video Path: {row.get('video_path', row.get('source_video', 'N/A'))}\")\n",
    "    print(f\"  Start Time: {row['start_time']:.2f}s\")\n",
    "    \n",
    "    if spatial_meta:\n",
    "        print(f\"\\n  Spatial Metadata:\")\n",
    "        for key, val in spatial_meta.items():\n",
    "            if key in ['azimuth', 'elevation']:\n",
    "                print(f\"    {key}: {val:.4f} rad ({np.degrees(val):.1f} deg)\")\n",
    "            else:\n",
    "                print(f\"    {key}: {val:.4f}\")\n",
    "    \n",
    "    # Prepare batch (add batch dimension)\n",
    "    mixed_batch = mixed_audio.unsqueeze(0).to(DEVICE)\n",
    "    clean_batch = clean_audio.unsqueeze(0).to(DEVICE)\n",
    "    video_batch = video_tensor.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"\\nRunning model inference...\")\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            estimated_audio = test_model(mixed_batch, video_batch)\n",
    "    \n",
    "    # Move to CPU for visualization\n",
    "    estimated_audio = estimated_audio.squeeze(0).cpu()\n",
    "    mixed_audio = mixed_audio.cpu()\n",
    "    clean_audio = clean_audio.cpu()\n",
    "    \n",
    "    # Handle shape mismatch\n",
    "    if estimated_audio.shape[-1] != clean_audio.shape[-1]:\n",
    "        min_len = min(estimated_audio.shape[-1], clean_audio.shape[-1])\n",
    "        estimated_audio = estimated_audio[..., :min_len]\n",
    "        clean_audio = clean_audio[..., :min_len]\n",
    "        mixed_audio = mixed_audio[..., :min_len]\n",
    "    \n",
    "    # Calculate SI-SNR improvement\n",
    "    with torch.no_grad():\n",
    "        # Use consistent reference: average 4-ch clean to mono if needed\n",
    "        clean_mono = clean_audio.mean(dim=0, keepdim=True) if clean_audio.dim() == 2 else clean_audio\n",
    "        mixed_mono = mixed_audio.mean(dim=0, keepdim=True) if mixed_audio.dim() == 2 else mixed_audio[0].unsqueeze(0)\n",
    "        \n",
    "        _, input_si_snr = si_snr_loss(mixed_mono, clean_mono.unsqueeze(0))\n",
    "        _, output_si_snr = si_snr_loss(estimated_audio, clean_mono.unsqueeze(0))\n",
    "        si_snr_improvement = output_si_snr - input_si_snr\n",
    "    \n",
    "    print(f\"\\n--- METRICS ---\")\n",
    "    print(f\"Input SI-SNR:  {input_si_snr:.2f} dB\")\n",
    "    print(f\"Output SI-SNR: {output_si_snr:.2f} dB\")\n",
    "    print(f\"SI-SNR Improvement: {si_snr_improvement:.2f} dB\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # VISUALIZATION\n",
    "    # ============================================================\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(4, 2, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    time_axis = np.arange(mixed_audio.shape[1]) / 16000\n",
    "    \n",
    "    # --- Waveforms ---\n",
    "    # Mixed Audio (Mic 1)\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.plot(time_axis, mixed_audio[0].numpy(), linewidth=0.5, color='blue')\n",
    "    ax1.set_title('Input: Mixed Audio (Microphone 1)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Amplitude')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim([0, time_axis[-1]])\n",
    "    \n",
    "    # Estimated Audio\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    ax2.plot(time_axis, estimated_audio[0].numpy(), linewidth=0.5, color='red')\n",
    "    ax2.set_title('Model Output: Separated Speech', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Amplitude')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim([0, time_axis[-1]])\n",
    "    \n",
    "    # Clean Audio (Ground Truth)\n",
    "    ax3 = fig.add_subplot(gs[2, :])\n",
    "    ax3.plot(time_axis, clean_audio[0].numpy(), linewidth=0.5, color='green')\n",
    "    ax3.set_title('Ground Truth: Clean Speech', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Amplitude')\n",
    "    ax3.set_xlabel('Time (s)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xlim([0, time_axis[-1]])\n",
    "    \n",
    "    # --- Spectrograms ---\n",
    "    # Input Spectrogram\n",
    "    ax4 = fig.add_subplot(gs[3, 0])\n",
    "    D_mixed = librosa.amplitude_to_db(np.abs(librosa.stft(mixed_audio[0].numpy())), ref=np.max)\n",
    "    librosa.display.specshow(D_mixed, sr=16000, x_axis='time', y_axis='hz', ax=ax4, cmap='viridis')\n",
    "    ax4.set_title('Input Spectrogram', fontsize=11)\n",
    "    ax4.set_ylim([0, 8000])\n",
    "    \n",
    "    # Output Spectrogram\n",
    "    ax5 = fig.add_subplot(gs[3, 1])\n",
    "    D_estimated = librosa.amplitude_to_db(np.abs(librosa.stft(estimated_audio[0].numpy())), ref=np.max)\n",
    "    librosa.display.specshow(D_estimated, sr=16000, x_axis='time', y_axis='hz', ax=ax5, cmap='viridis')\n",
    "    ax5.set_title('Output Spectrogram', fontsize=11)\n",
    "    ax5.set_ylim([0, 8000])\n",
    "    \n",
    "    # Add spatial info to title if available\n",
    "    if spatial_meta:\n",
    "        az_deg = np.degrees(spatial_meta.get('azimuth', 0))\n",
    "        el_deg = np.degrees(spatial_meta.get('elevation', 0))\n",
    "        title_extra = f' | Az: {az_deg:.1f}deg, El: {el_deg:.1f}deg'\n",
    "    else:\n",
    "        title_extra = ''\n",
    "    \n",
    "    plt.suptitle(f'Model Inference Test | SI-SNR Improvement: {si_snr_improvement:.2f} dB{title_extra}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.savefig(f\"inference_test_result.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nVisualization saved to: inference_test_result.png\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"AUDIO PLAYBACK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a464db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play Input Mixed Audio (Mic 1)\n",
    "print(\"\\n🔊 Input: Mixed Audio (Microphone 1)\")\n",
    "ipd.display(ipd.Audio(mixed_audio[0].numpy(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play Model Output\n",
    "print(\"\\n🔊 Output: Model Separated Speech\")\n",
    "ipd.display(ipd.Audio(estimated_audio[0].numpy(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737df8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play Ground Truth Clean Audio\n",
    "print(\"\\n🔊 Ground Truth: Clean Speech\")\n",
    "ipd.display(ipd.Audio(clean_audio[0].numpy(), rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370766f",
   "metadata": {},
   "source": [
    "## Test Set Evaluation\n",
    "\n",
    "Evaluate the best trained model on the held-out test set. Compute SI-SNR improvement statistics across all test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST SET EVALUATION\n",
    "# ============================================================\n",
    "# Evaluate the best model on held-out test set\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load test dataset\n",
    "print(f\"\\nLoading test dataset from: {TEST_CSV}\")\n",
    "test_ds = IsoNetDataset(\n",
    "    TEST_CSV,\n",
    "    max_samples=None,  # Use full test set\n",
    "    video_size=VIDEO_SIZE,\n",
    "    augment=False,     # No augmentation for test (deterministic)\n",
    "    jitter_pct=0.0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Test samples: {len(test_ds)} | Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Load best model\n",
    "print(f\"\\nLoading best model from: {CHECKPOINT_DIR}/best_model.pth\")\n",
    "best_model_path = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
    "\n",
    "if not os.path.exists(best_model_path):\n",
    "    print(f\"ERROR: Best model not found at {best_model_path}\")\n",
    "    print(\"Please train the model first!\")\n",
    "else:\n",
    "    # Initialize and load model\n",
    "    test_model = IsoNet(use_checkpointing=False).to(DEVICE)\n",
    "    checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"  Trained for: {checkpoint['epoch']} epochs\")\n",
    "    print(f\"  Val SI-SNR: {checkpoint.get('val_si_snr', -checkpoint['val_loss']):.2f} dB\")\n",
    "    \n",
    "    # Collect metrics for all test samples\n",
    "    all_input_si_snr = []\n",
    "    all_output_si_snr = []\n",
    "    all_si_snr_improvement = []\n",
    "    test_losses = []\n",
    "    \n",
    "    print(f\"\\nRunning evaluation on test set...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(tqdm.tqdm(test_loader, desc=\"Testing\")):\n",
    "            try:\n",
    "                if len(batch_data) == 4:\n",
    "                    mixed, clean, video, _ = batch_data\n",
    "                else:\n",
    "                    mixed, clean, video = batch_data\n",
    "                \n",
    "                mixed = mixed.to(DEVICE, non_blocking=True)\n",
    "                clean = clean.to(DEVICE, non_blocking=True)\n",
    "                video = video.to(DEVICE, non_blocking=True)\n",
    "                \n",
    "                with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                    estimate = test_model(mixed, video)\n",
    "                    \n",
    "                    # Handle shape mismatch\n",
    "                    if estimate.shape[-1] != clean.shape[-1]:\n",
    "                        min_len = min(estimate.shape[-1], clean.shape[-1])\n",
    "                        estimate = estimate[..., :min_len]\n",
    "                        clean = clean[..., :min_len]\n",
    "                        mixed = mixed[..., :min_len]\n",
    "                    \n",
    "                    # Compute loss (unpack tuple: si_snr_loss returns (loss, si_snr))\n",
    "                    loss, _ = si_snr_loss(estimate.squeeze(1), clean.squeeze(1))\n",
    "                    test_losses.append(loss.item())\n",
    "                    \n",
    "                    # Compute SI-SNR for each sample in batch\n",
    "                    for b in range(mixed.shape[0]):\n",
    "                        # Input SI-SNR (mic 1 vs clean) - unpack tuple\n",
    "                        _, input_snr = si_snr_loss(\n",
    "                            mixed[b, 0:1, :], \n",
    "                            clean[b, :, :]\n",
    "                        )\n",
    "                        \n",
    "                        # Output SI-SNR (estimate vs clean) - unpack tuple\n",
    "                        _, output_snr = si_snr_loss(\n",
    "                            estimate[b, :, :], \n",
    "                            clean[b, :, :]\n",
    "                        )\n",
    "                        \n",
    "                        improvement = output_snr - input_snr\n",
    "                        \n",
    "                        all_input_si_snr.append(input_snr)\n",
    "                        all_output_si_snr.append(output_snr)\n",
    "                        all_si_snr_improvement.append(improvement)\n",
    "                        \n",
    "            except RuntimeError as e:\n",
    "                print(f\"[!] Batch {batch_idx} error: {str(e)[:50]}\")\n",
    "                continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_input_si_snr = np.array(all_input_si_snr)\n",
    "    all_output_si_snr = np.array(all_output_si_snr)\n",
    "    all_si_snr_improvement = np.array(all_si_snr_improvement)\n",
    "    test_losses = np.array(test_losses)\n",
    "    \n",
    "    # Compute statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEST RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n{'Metric':<25} {'Mean':>12} {'Std':>12} {'Min':>12} {'Max':>12}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Input SI-SNR (dB)':<25} {all_input_si_snr.mean():>12.2f} {all_input_si_snr.std():>12.2f} {all_input_si_snr.min():>12.2f} {all_input_si_snr.max():>12.2f}\")\n",
    "    print(f\"{'Output SI-SNR (dB)':<25} {all_output_si_snr.mean():>12.2f} {all_output_si_snr.std():>12.2f} {all_output_si_snr.min():>12.2f} {all_output_si_snr.max():>12.2f}\")\n",
    "    print(f\"{'SI-SNR Improvement (dB)':<25} {all_si_snr_improvement.mean():>12.2f} {all_si_snr_improvement.std():>12.2f} {all_si_snr_improvement.min():>12.2f} {all_si_snr_improvement.max():>12.2f}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\n--- SUMMARY ---\")\n",
    "    print(f\"  Test Samples: {len(all_si_snr_improvement)}\")\n",
    "    print(f\"  Average Test Loss: {test_losses.mean():.4f}\")\n",
    "    print(f\"  Average Input SI-SNR: {all_input_si_snr.mean():.2f} dB\")\n",
    "    print(f\"  Average Output SI-SNR: {all_output_si_snr.mean():.2f} dB\")\n",
    "    print(f\"  Average SI-SNR Improvement: {all_si_snr_improvement.mean():.2f} dB\")\n",
    "    print(f\"  Samples with Improvement: {(all_si_snr_improvement > 0).sum()}/{len(all_si_snr_improvement)} ({100*(all_si_snr_improvement > 0).mean():.1f}%)\")\n",
    "    \n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6619098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST RESULTS VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. SI-SNR Improvement Distribution\n",
    "axes[0].hist(all_si_snr_improvement, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='No Improvement')\n",
    "axes[0].axvline(x=all_si_snr_improvement.mean(), color='green', linestyle='-', linewidth=2, label=f'Mean: {all_si_snr_improvement.mean():.2f} dB')\n",
    "axes[0].set_xlabel('SI-SNR Improvement (dB)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('SI-SNR Improvement Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Input vs Output SI-SNR Scatter\n",
    "axes[1].scatter(all_input_si_snr, all_output_si_snr, alpha=0.5, s=10)\n",
    "min_val = min(all_input_si_snr.min(), all_output_si_snr.min()) - 2\n",
    "max_val = max(all_input_si_snr.max(), all_output_si_snr.max()) + 2\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='No Change Line')\n",
    "axes[1].set_xlabel('Input SI-SNR (dB)')\n",
    "axes[1].set_ylabel('Output SI-SNR (dB)')\n",
    "axes[1].set_title('Input vs Output SI-SNR')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([min_val, max_val])\n",
    "axes[1].set_ylim([min_val, max_val])\n",
    "\n",
    "# 3. Cumulative Distribution\n",
    "sorted_improvements = np.sort(all_si_snr_improvement)\n",
    "cdf = np.arange(1, len(sorted_improvements) + 1) / len(sorted_improvements)\n",
    "axes[2].plot(sorted_improvements, cdf * 100, linewidth=2, color='steelblue')\n",
    "axes[2].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[2].axhline(y=50, color='gray', linestyle=':', linewidth=1)\n",
    "axes[2].set_xlabel('SI-SNR Improvement (dB)')\n",
    "axes[2].set_ylabel('Cumulative % of Samples')\n",
    "axes[2].set_title('Cumulative Distribution of Improvement')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Test Set Results | Mean Improvement: {all_si_snr_improvement.mean():.2f} dB | {len(all_si_snr_improvement)} samples', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"test_results_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to: test_results_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a07da",
   "metadata": {},
   "source": [
    "## Cross-Video Testing: Same Audio + Different Video Selections\n",
    "\n",
    "This test demonstrates the core multimodal speaker isolation capability:\n",
    "- **Same mixed audio** (containing multiple speakers)\n",
    "- **Different video inputs** (showing different target speakers)\n",
    "- **Expected result**: Different isolated outputs based on who is shown in the video\n",
    "\n",
    "This proves that the model uses visual cues to select which speaker to isolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3576e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CROSS-VIDEO TESTING: Same Audio + Different Videos\n",
    "# ============================================================\n",
    "# Test how model responds to different video inputs with same audio\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CROSS-VIDEO TESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load test dataset for cross-video testing\n",
    "cross_test_ds = IsoNetDataset(\n",
    "    TEST_CSV,\n",
    "    max_samples=10,  # Load a few samples for cross-testing\n",
    "    video_size=VIDEO_SIZE,\n",
    "    augment=False,\n",
    "    jitter_pct=0.0\n",
    ")\n",
    "\n",
    "# Load best model if not already loaded\n",
    "if 'test_model' not in locals():\n",
    "    print(\"Loading best model...\")\n",
    "    test_model = IsoNet(use_checkpointing=False).to(DEVICE)\n",
    "    checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pth\", map_location=DEVICE)\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_model.eval()\n",
    "    print(f\"Model loaded from epoch {checkpoint['epoch']}\")\n",
    "\n",
    "# Select a base sample (audio to isolate from)\n",
    "base_idx = 0\n",
    "base_mixed, base_clean, base_video, base_meta = cross_test_ds[base_idx]\n",
    "base_row = cross_test_ds.meta.iloc[base_idx]\n",
    "\n",
    "print(f\"\\nBase Mixed Audio: {base_row['filename']}\")\n",
    "print(f\"Base Audio Shape: {base_mixed.shape}\")\n",
    "print(f\"\\nThis audio will be tested with videos from {min(5, len(cross_test_ds))} different samples\")\n",
    "\n",
    "# Test with multiple different videos\n",
    "num_videos_to_test = min(5, len(cross_test_ds))\n",
    "cross_results = []\n",
    "\n",
    "for video_idx in range(num_videos_to_test):\n",
    "    # Get video from a different sample\n",
    "    _, _, test_video, _ = cross_test_ds[video_idx]\n",
    "    test_row = cross_test_ds.meta.iloc[video_idx]\n",
    "    \n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Test {video_idx + 1}/{num_videos_to_test}: Using video from {test_row['filename']}\")\n",
    "    \n",
    "    # Use base_mixed audio + test_video (potentially from different speaker)\n",
    "    mixed_batch = base_mixed.unsqueeze(0).to(DEVICE)\n",
    "    video_batch = test_video.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            isolated = test_model(mixed_batch, video_batch)\n",
    "    \n",
    "    isolated = isolated.squeeze(0).cpu()\n",
    "    \n",
    "    # Handle shape mismatch\n",
    "    if isolated.shape[-1] != base_clean.shape[-1]:\n",
    "        min_len = min(isolated.shape[-1], base_clean.shape[-1])\n",
    "        isolated = isolated[..., :min_len]\n",
    "    \n",
    "    # Calculate metrics (unpack the tuple properly)\n",
    "    output_rms = torch.sqrt(torch.mean(isolated[0] ** 2)).item()\n",
    "    loss_val, si_snr_val = si_snr_loss(isolated, base_clean[..., :isolated.shape[-1]])\n",
    "    output_si_snr = si_snr_val  # Already positive SI-SNR in dB\n",
    "    \n",
    "    # Check if this is the matching video (same sample)\n",
    "    is_matching = (video_idx == base_idx)\n",
    "    match_str = \" [MATCHING VIDEO]\" if is_matching else \"\"\n",
    "    \n",
    "    print(f\"  Output RMS: {output_rms:.6f} | SI-SNR: {output_si_snr:.2f} dB{match_str}\")\n",
    "    \n",
    "    cross_results.append({\n",
    "        'video_idx': video_idx,\n",
    "        'video_filename': test_row['filename'],\n",
    "        'video_frames': test_video,\n",
    "        'isolated_audio': isolated,\n",
    "        'output_rms': output_rms,\n",
    "        'output_si_snr': output_si_snr,\n",
    "        'is_matching': is_matching\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Cross-video testing complete!\")\n",
    "print(f\"Same mixed audio produced {num_videos_to_test} different outputs\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CROSS-VIDEO VISUALIZATION: Waveforms & Spectrograms\n",
    "# ============================================================\n",
    "\n",
    "num_videos = len(cross_results)\n",
    "fig, axes = plt.subplots(num_videos, 2, figsize=(16, 3 * num_videos))\n",
    "\n",
    "if num_videos == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for test_num, result in enumerate(cross_results):\n",
    "    isolated = result['isolated_audio'][0].numpy()\n",
    "    time_axis = np.arange(len(isolated)) / 16000\n",
    "    \n",
    "    # Color based on matching\n",
    "    color = 'green' if result['is_matching'] else f'C{test_num}'\n",
    "    title_suffix = \" [MATCHING]\" if result['is_matching'] else \"\"\n",
    "    \n",
    "    # Waveform\n",
    "    axes[test_num, 0].plot(time_axis, isolated, linewidth=0.5, color=color)\n",
    "    axes[test_num, 0].set_title(\n",
    "        f'Output {test_num+1}: {result[\"video_filename\"][:20]}...{title_suffix}\\n'\n",
    "        f'SI-SNR: {result[\"output_si_snr\"]:.2f} dB | RMS: {result[\"output_rms\"]:.4f}', \n",
    "        fontsize=10, fontweight='bold'\n",
    "    )\n",
    "    axes[test_num, 0].set_ylabel('Amplitude')\n",
    "    axes[test_num, 0].grid(True, alpha=0.3)\n",
    "    axes[test_num, 0].set_xlim([0, time_axis[-1]])\n",
    "    \n",
    "    if test_num == num_videos - 1:\n",
    "        axes[test_num, 0].set_xlabel('Time (s)')\n",
    "    \n",
    "    # Spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(isolated)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=16000, x_axis='time', y_axis='hz', \n",
    "                            ax=axes[test_num, 1], cmap='viridis')\n",
    "    axes[test_num, 1].set_title(f'Output {test_num+1} Spectrogram', fontsize=10, fontweight='bold')\n",
    "    axes[test_num, 1].set_ylim([0, 8000])\n",
    "\n",
    "plt.suptitle(f'Cross-Video Test: Same Audio ({base_row[\"filename\"]}) + Different Video Selections', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cross_video_output_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Cross-video visualization saved: cross_video_output_comparison.png\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-VIDEO TEST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Video':<35} {'SI-SNR (dB)':<15} {'RMS':<12} {'Match'}\")\n",
    "print(\"-\" * 80)\n",
    "for result in cross_results:\n",
    "    match_str = \"YES\" if result['is_matching'] else \"no\"\n",
    "    print(f\"{result['video_filename'][:32]:<35} {result['output_si_snr']:>12.2f}   {result['output_rms']:>10.6f}   {match_str}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate differences\n",
    "si_snr_values = [r['output_si_snr'] for r in cross_results]\n",
    "rms_values = [r['output_rms'] for r in cross_results]\n",
    "print(f\"\\nSI-SNR Range: {min(si_snr_values):.2f} to {max(si_snr_values):.2f} dB (diff: {max(si_snr_values)-min(si_snr_values):.2f} dB)\")\n",
    "print(f\"RMS Range: {min(rms_values):.6f} to {max(rms_values):.6f}\")\n",
    "print(f\"\\nObservation: Different videos → Different isolated outputs!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c833008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE VIDEO FRAMES USED FOR SELECTION\n",
    "# ============================================================\n",
    "\n",
    "num_videos = len(cross_results)\n",
    "fig, axes = plt.subplots(num_videos + 1, 5, figsize=(15, 3 * (num_videos + 1)))\n",
    "\n",
    "# Top row: Base mixed audio waveform\n",
    "time_axis = np.arange(base_mixed.shape[1]) / 16000\n",
    "for col in range(5):\n",
    "    if col == 2:  # Center subplot for waveform\n",
    "        axes[0, col].plot(time_axis, base_mixed[0].numpy(), linewidth=0.5, color='blue')\n",
    "        axes[0, col].set_title(f'Base Mixed Audio\\n{base_row[\"filename\"][:25]}...', \n",
    "                               fontsize=9, fontweight='bold')\n",
    "        axes[0, col].set_xlabel('Time (s)', fontsize=8)\n",
    "        axes[0, col].set_ylabel('Amplitude', fontsize=8)\n",
    "        axes[0, col].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, col].axis('off')\n",
    "\n",
    "# Subsequent rows: Video frames for each test\n",
    "for test_num, result in enumerate(cross_results):\n",
    "    video = result['video_frames']\n",
    "    sample_frames = np.linspace(0, video.shape[1]-1, 5, dtype=int)\n",
    "    \n",
    "    border_color = 'green' if result['is_matching'] else 'gray'\n",
    "    \n",
    "    for col, frame_num in enumerate(sample_frames):\n",
    "        frame = video[:, frame_num, :, :].permute(1, 2, 0).numpy()\n",
    "        axes[test_num + 1, col].imshow(frame)\n",
    "        axes[test_num + 1, col].axis('off')\n",
    "        \n",
    "        # Add colored border for matching video\n",
    "        if result['is_matching']:\n",
    "            for spine in axes[test_num + 1, col].spines.values():\n",
    "                spine.set_edgecolor('green')\n",
    "                spine.set_linewidth(3)\n",
    "                spine.set_visible(True)\n",
    "        \n",
    "        if col == 0:\n",
    "            match_str = \" [MATCH]\" if result['is_matching'] else \"\"\n",
    "            axes[test_num + 1, col].set_ylabel(\n",
    "                f\"Video {test_num+1}{match_str}\\n{result['video_filename'][:12]}...\", \n",
    "                fontsize=9, fontweight='bold'\n",
    "            )\n",
    "\n",
    "plt.suptitle(f'Cross-Video Test: Same Audio + Different Video Selections\\n'\n",
    "             f'Base Audio: {base_row[\"filename\"]}', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cross_video_frames.png\", dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Cross-video frames visualization saved: cross_video_frames.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
