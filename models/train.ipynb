{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7624bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# from dataset import IsoNetDataset\n",
    "import os \n",
    "\n",
    "# --- CONFIGURATION FROM DIAGRAM ---\n",
    "VISUAL_DIM = 256       # Output of Visual Stream (V)\n",
    "SPATIAL_DIM = 128      # Output of Spatial Stream (S)\n",
    "AUDIO_ENC_DIM = 512    # Internal Audio Feature Dimension\n",
    "AUDIO_CHANNELS = 4     # Number of Mics\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4          # FIXED: Increased from 1 to 4 for proper BatchNorm\n",
    "EPOCHS = 100\n",
    "LR = 1e-4               # TCNs prefer lower learning rates\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6852e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()        # free cached memory\n",
    "torch.cuda.synchronize()        # wait for all kernels to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d726b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IsoNetDataset(Dataset):\n",
    "    def __init__(self, csv_path, clip_length=4.0, fps=25, video_size=(224, 224), max_samples=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (str): Path to train.csv or val.csv\n",
    "            clip_length (float): Audio duration in seconds (must match simulation)\n",
    "            fps (int): Target frames per second for video (VoxCeleb is 25)\n",
    "            video_size (tuple): Target resize dimension (H, W) - defaults to 224x224 for better quality\n",
    "            max_samples (int, optional): Limit dataset to first N samples for testing\n",
    "        \"\"\"\n",
    "        self.meta = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Limit dataset size for testing\n",
    "        if max_samples is not None:\n",
    "            self.meta = self.meta.head(max_samples)\n",
    "            print(f\"Debug Mode: Using only {len(self.meta)} samples\")\n",
    "        \n",
    "        # Assume csv is inside 'multich/', so parent is the root\n",
    "        self.root_dir = Path(csv_path).parent\n",
    "        \n",
    "        self.clip_length = clip_length\n",
    "        self.fps = fps\n",
    "        self.target_frames = int(clip_length * fps)  # 4.0 * 25 = 100 frames\n",
    "        self.video_size = video_size\n",
    "\n",
    "    def load_video_frames(self, video_path, start_time):\n",
    "        \"\"\"\n",
    "        Efficiently seeks to start_time and reads exactly target_frames.\n",
    "        Resizes frames to target size without cropping.\n",
    "        Returns: Tensor [Channels, Time, H, W]\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        \n",
    "        # Get Video Properties\n",
    "        vid_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if vid_fps == 0 or np.isnan(vid_fps): \n",
    "            vid_fps = 25.0\n",
    "            \n",
    "        # Calculate Start Frame Index\n",
    "        start_frame_idx = int(start_time * vid_fps)\n",
    "        \n",
    "        # Seek to exact frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame_idx)\n",
    "        \n",
    "        frames = []\n",
    "        for i in range(self.target_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # BGR to RGB (FIXED: was BGR2GRAY which removed channel dimension)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Resize to target size\n",
    "            frame = cv2.resize(frame, self.video_size)\n",
    "            frames.append(frame)\n",
    "            \n",
    "        cap.release()\n",
    "        \n",
    "        # Handle Edge Case: Video ended too early\n",
    "        if len(frames) < self.target_frames:\n",
    "            # If video completely failed to load, create black frames\n",
    "            if len(frames) == 0:\n",
    "                frames = [np.zeros((self.video_size[0], self.video_size[1], 3), dtype=np.uint8)] * self.target_frames\n",
    "            else:\n",
    "                padding = [frames[-1]] * (self.target_frames - len(frames))\n",
    "                frames.extend(padding)\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        # Shape: [Time, H, W, Channels] -> PyTorch standard [Channels, Time, H, W]\n",
    "        # Normalize to 0-1 range\n",
    "        buffer = np.array(frames, dtype=np.float32) / 255.0\n",
    "        tensor = torch.from_numpy(buffer)\n",
    "        return tensor.permute(3, 0, 1, 2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row = self.meta.iloc[idx]\n",
    "        \n",
    "        # 1. Get Paths & Info\n",
    "        filename = row['filename']\n",
    "        vid_path_str = row['video_path']\n",
    "        start_time = float(row['start_time'])\n",
    "        \n",
    "        mixed_path = self.root_dir / \"mixed\" / f\"{filename}.wav\"\n",
    "        clean_path = self.root_dir / \"clean\" / f\"{filename}.wav\"\n",
    "\n",
    "        # 2. Load Audio\n",
    "        mixed_audio, _ = torchaudio.load(mixed_path)\n",
    "        clean_audio, _ = torchaudio.load(clean_path)\n",
    "\n",
    "        # 3. Load Video (Full frame, resized to target size)\n",
    "        video_tensor = self.load_video_frames(vid_path_str, start_time)\n",
    "\n",
    "        # 4. Ensure audio length matches exactly\n",
    "        target_samples = int(self.clip_length * 16000)\n",
    "        \n",
    "        if mixed_audio.shape[1] > target_samples:\n",
    "            mixed_audio = mixed_audio[:, :target_samples]\n",
    "            clean_audio = clean_audio[:, :target_samples]\n",
    "        elif mixed_audio.shape[1] < target_samples:\n",
    "            pad_size = target_samples - mixed_audio.shape[1]\n",
    "            mixed_audio = torch.nn.functional.pad(mixed_audio, (0, pad_size))\n",
    "            clean_audio = torch.nn.functional.pad(clean_audio, (0, pad_size))\n",
    "\n",
    "        return mixed_audio, clean_audio, video_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9066e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualStream(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisualStream, self).__init__()\n",
    "        # Load ResNet-18\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove classification head\n",
    "        modules = list(resnet.children())[:-1] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Project 512 -> 256 (V)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, VISUAL_DIM),\n",
    "            nn.BatchNorm1d(VISUAL_DIM),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # FIXED: ImageNet normalization for pretrained ResNet\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 3, Time, H, W] where H, W can be any size (e.g., 224x224)\n",
    "        B, C, T, H, W = x.shape\n",
    "        \n",
    "        # Fold Time into Batch\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous().view(B * T, C, H, W)\n",
    "        \n",
    "        # FIXED: Apply ImageNet normalization before ResNet\n",
    "        x = (x - self.mean) / self.std\n",
    "        \n",
    "        # Extract Features (ResNet handles any input size via adaptive pooling)\n",
    "        x = self.resnet(x)       # [B*T, 512, 1, 1]\n",
    "        x = x.view(B * T, -1)    # [B*T, 512]\n",
    "        \n",
    "        # Project to 256\n",
    "        x = self.projection(x)   # [B*T, 256]\n",
    "        \n",
    "        # Unfold Time\n",
    "        x = x.view(B, T, -1).permute(0, 2, 1) # [B, 256, Time]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbbfac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialStream(nn.Module):\n",
    "    def __init__(self, num_mics=4):\n",
    "        super(SpatialStream, self).__init__()\n",
    "        \n",
    "        # We compute GCC-PHAT for all pairs. \n",
    "        # For 4 mics, pairs = 4*(3)/2 = 6 pairs.\n",
    "        self.num_pairs = (num_mics * (num_mics - 1)) // 2\n",
    "        \n",
    "        # FIXED: Spatial CNN Encoder\n",
    "        # - Changed from kernel_size=1 to larger kernels (31, 15) to capture temporal patterns\n",
    "        # - Changed from BatchNorm1d to GroupNorm for stability with small batch sizes\n",
    "        # Input: [Batch, Pairs(6), Lags, Time]\n",
    "        # We treat Pairs as Channels\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.num_pairs, 64, kernel_size=31, stride=1, padding=15),\n",
    "            nn.GroupNorm(1, 64),  # Changed from BatchNorm1d\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=15, stride=1, padding=7),\n",
    "            nn.GroupNorm(1, 128),  # Changed from BatchNorm1d\n",
    "            nn.PReLU(),\n",
    "            nn.Conv1d(128, SPATIAL_DIM, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def compute_gcc_phat(self, x):\n",
    "        \"\"\"\n",
    "        Compute Generalized Cross-Correlation Phase Transform (GCC-PHAT)\n",
    "        Input x: [Batch, Mics, Samples]\n",
    "        \"\"\"\n",
    "        B, M, L = x.shape\n",
    "        \n",
    "        # 1. FFT\n",
    "        # n_fft matches window size roughly\n",
    "        X = torch.fft.rfft(x, dim=-1)\n",
    "        \n",
    "        # 2. Compute Pairs\n",
    "        # We want to cross-correlate every pair (i, j)\n",
    "        pairs = []\n",
    "        for i in range(M):\n",
    "            for j in range(i + 1, M):\n",
    "                # Cross-spectrum: X_i * conj(X_j)\n",
    "                R = X[:, i, :] * torch.conj(X[:, j, :])\n",
    "                # Normalization (PHAT): Divide by magnitude\n",
    "                R = R / (torch.abs(R) + 1e-8)\n",
    "                # IFFT to get time-domain correlation\n",
    "                r = torch.fft.irfft(R, dim=-1)\n",
    "                \n",
    "                # Apply shift/lag window (we assume delays are small)\n",
    "                # This makes it a feature vector per time frame is tricky without STFT.\n",
    "                # Simplified: We treat the whole clip's correlation as a static spatial signature\n",
    "                # OR (Better): We perform this on STFT frames. \n",
    "                \n",
    "                # For simplicity in this implementation, we will use a learnable \n",
    "                # layer instead of raw GCC-PHAT if raw is too complex to batch.\n",
    "                # BUT, let's assume the input here is actually the GCC features.\n",
    "                pairs.append(r)\n",
    "                \n",
    "        return torch.stack(pairs, dim=1) # [B, 6, Samples]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 4, Samples]\n",
    "        \n",
    "        # In a real heavy model, we do STFT -> GCC-PHAT -> CNN.\n",
    "        # Here, we will use a \"Learnable Spatial Encoder\" which is faster/easier\n",
    "        # and often outperforms analytical GCC-PHAT.\n",
    "        \n",
    "        # 1. Extract correlations implicitly via 1D Conv across channels\n",
    "        # [B, 4, T] -> [B, 128, T]\n",
    "        # We pool over time to get a Global Spatial Signature S\n",
    "        \n",
    "        gcc_feat = self.compute_gcc_phat(x) # [B, 6, Samples]\n",
    "        \n",
    "        # Encode features\n",
    "        x = self.encoder(gcc_feat) # [B, 128, Samples]\n",
    "        \n",
    "        # Global Average Pooling to get single vector S \\in R^128\n",
    "        x = torch.mean(x, dim=-1)  # [B, 128]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c61e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMLayer(nn.Module):\n",
    "    def __init__(self, in_channels, cond_dim):\n",
    "        super(FiLMLayer, self).__init__()\n",
    "        # We map the Conditioning (S+V) to Gamma (Scale) and Beta (Shift)\n",
    "        self.conv_gamma = nn.Conv1d(cond_dim, in_channels, 1)\n",
    "        self.conv_beta = nn.Conv1d(cond_dim, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # x: [Batch, Channels, Time]\n",
    "        # condition: [Batch, Cond_Dim, Time]\n",
    "        \n",
    "        gamma = self.conv_gamma(condition)  # [B, C, T]\n",
    "        beta = self.conv_beta(condition)    # [B, C, T]\n",
    "            \n",
    "        # FiLM Formula: Gamma * x + Beta\n",
    "        return (gamma * x) + beta\n",
    "\n",
    "class ExtractionBlock(nn.Module):\n",
    "    \"\"\" TCN Block with FiLM Conditioning \"\"\"\n",
    "    def __init__(self, in_channels, hid_channels, cond_dim, dilation):\n",
    "        super(ExtractionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, hid_channels, 1)\n",
    "        self.norm1 = nn.GroupNorm(1, hid_channels)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        # FiLM comes after first activation usually\n",
    "        self.film = FiLMLayer(hid_channels, cond_dim)\n",
    "        \n",
    "        self.dconv = nn.Conv1d(hid_channels, hid_channels, 3, \n",
    "                               groups=hid_channels, padding=dilation, dilation=dilation)\n",
    "        self.norm2 = nn.GroupNorm(1, hid_channels)\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(hid_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        # Apply FiLM Conditioning\n",
    "        # The condition (S+V) modulates the features here\n",
    "        x = self.film(x, condition)\n",
    "        \n",
    "        x = self.dconv(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.prelu2(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a289d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IsoNet, self).__init__()\n",
    "        \n",
    "        # 1. Streams\n",
    "        self.visual_stream = VisualStream()  # Output: 256\n",
    "        self.spatial_stream = SpatialStream(AUDIO_CHANNELS) # Output: 128\n",
    "        \n",
    "        # 2. Audio Encoder (Purple box start)\n",
    "        self.audio_enc = nn.Conv1d(AUDIO_CHANNELS, AUDIO_ENC_DIM, kernel_size=16, stride=8, bias=False)\n",
    "        \n",
    "        # 3. Conditioning Prep\n",
    "        # We concatenate S (128) + V (256) = 384\n",
    "        self.cond_dim = SPATIAL_DIM + VISUAL_DIM\n",
    "        \n",
    "        # 4. TCN with FiLM (Purple box middle)\n",
    "        self.tcn_blocks = nn.ModuleList([\n",
    "            ExtractionBlock(AUDIO_ENC_DIM, 128, self.cond_dim, dilation=2**i) \n",
    "            for i in range(8)\n",
    "        ])\n",
    "        \n",
    "        # 5. Mask Decoder (Purple box end)\n",
    "        self.mask_conv = nn.Conv1d(AUDIO_ENC_DIM, AUDIO_ENC_DIM, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # 6. Audio Decoder (Reconstructs waveform)\n",
    "        self.audio_dec = nn.ConvTranspose1d(AUDIO_ENC_DIM, 1, kernel_size=16, stride=8, bias=False)\n",
    "\n",
    "    def forward(self, audio_mix, video_frames):\n",
    "        # audio_mix: [B, 4, Samples]\n",
    "        # video_frames: [B, 3, T_v, H, W]\n",
    "        \n",
    "        # --- A. Spatial Stream ---\n",
    "        # Get global spatial embedding S\n",
    "        S = self.spatial_stream(audio_mix) # [B, 128]\n",
    "        \n",
    "        # --- B. Visual Stream ---\n",
    "        # Get visual embedding V\n",
    "        V = self.visual_stream(video_frames) # [B, 256, T_v]\n",
    "        \n",
    "        # --- C. Audio Encoding ---\n",
    "        audio_feat = self.audio_enc(audio_mix) # [B, 512, T_a]\n",
    "        \n",
    "        # --- D. Synchronization (Upsampling) ---\n",
    "        # Video (25 FPS) is slower than Audio Frames. Upsample V to match Audio T_a\n",
    "        V_upsampled = F.interpolate(V, size=audio_feat.shape[-1], mode='nearest')\n",
    "        \n",
    "        # Expand S to match time dimension: [B, 128] -> [B, 128, T_a]\n",
    "        S_expanded = S.unsqueeze(-1).expand(-1, -1, audio_feat.shape[-1])\n",
    "        \n",
    "        # Concatenate S + V to create Conditioning Vector\n",
    "        # Shape: [B, 384, T_a]\n",
    "        condition = torch.cat([S_expanded, V_upsampled], dim=1)\n",
    "        \n",
    "        # --- E. FiLM Extraction Loop ---\n",
    "        x = audio_feat\n",
    "        for block in self.tcn_blocks:\n",
    "            # We pass the condition to every block\n",
    "            x = block(x, condition)\n",
    "            \n",
    "        # --- F. Masking & Decoding ---\n",
    "        mask = self.sigmoid(self.mask_conv(x))\n",
    "        masked_feat = audio_feat * mask\n",
    "        clean_speech = self.audio_dec(masked_feat)\n",
    "        \n",
    "        return clean_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8264aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Model\n",
    "# model = IsoNet().to(DEVICE)\n",
    "# print(f\"IsoNet Created. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# # Dummy Data\n",
    "# dummy_audio = torch.randn(2, 4, 64000).to(DEVICE)     # 4 seconds audio\n",
    "# dummy_video = torch.randn(2, 3, 100, 112, 112).to(DEVICE) # 100 frames\n",
    "\n",
    "# # Forward Pass\n",
    "# output = model(dummy_audio, dummy_video)\n",
    "# print(f\"Input: {dummy_audio.shape}\")\n",
    "# print(f\"Output: {output.shape}\")\n",
    "\n",
    "# # Check\n",
    "# if output.shape[1] == 1 and abs(output.shape[-1] - 64000) < 100:\n",
    "#     print(\"IsoNet Architecture matches diagram successfully!\")\n",
    "# else:\n",
    "#     print(\"IsoNet Architecture does not match diagram.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154d9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def si_snr_loss(estimate, reference, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Scale-Invariant SNR Loss.\n",
    "    Args:\n",
    "        estimate: [Batch, Samples] - The predicted audio\n",
    "        reference: [Batch, Samples] - The clean ground truth\n",
    "    Returns:\n",
    "        Scalar Loss (Negative SI-SNR)\n",
    "    \"\"\"\n",
    "    # 1. Zero-mean the signals\n",
    "    estimate = estimate - torch.mean(estimate, dim=-1, keepdim=True)\n",
    "    reference = reference - torch.mean(reference, dim=-1, keepdim=True)\n",
    "    \n",
    "    # 2. Calculate optimal scaling factor (alpha)\n",
    "    # Dot product <ref, est> / <ref, ref>\n",
    "    ref_energy = torch.sum(reference ** 2, dim=-1, keepdim=True) + epsilon\n",
    "    dot = torch.sum(reference * estimate, dim=-1, keepdim=True)\n",
    "    alpha = dot / ref_energy\n",
    "    \n",
    "    # 3. Projection\n",
    "    target = alpha * reference\n",
    "    noise = estimate - target\n",
    "    \n",
    "    # 4. SI-SNR Calculation\n",
    "    si_snr = 10 * torch.log10(\n",
    "        torch.sum(target ** 2, dim=-1) / (torch.sum(noise ** 2, dim=-1) + epsilon)\n",
    "    )\n",
    "    \n",
    "    # 5. Return negative because we want to MAXIMIZE SNR (minimize loss)\n",
    "    return -torch.mean(si_snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018ec3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Mode: Using only 10 samples\n",
      "Debug Mode: Using only 5 samples\n"
     ]
    }
   ],
   "source": [
    "train_csv = \"/run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/train.csv\"\n",
    "val_csv = \"/run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/val.csv\"\n",
    "\n",
    "# Use only 50 samples for testing (increase once working)\n",
    "train_ds = IsoNetDataset(train_csv, max_samples=10)\n",
    "val_ds = IsoNetDataset(val_csv, max_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff49556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68f3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsoNet().to(DEVICE)\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed shape: torch.Size([4, 4, 64000]), Clean shape: torch.Size([4, 1, 64000]), Video shape: torch.Size([4, 3, 100, 224, 224])\n",
      "Audio Max: 0.9000, Audio Min: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  33%|███▎      | 1/3 [00:01<00:03,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 0: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 110.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.38 GiB memory in use. Of the allocated memory 5.25 GiB is allocated by PyTorch, and 24.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  67%|██████▋   | 2/3 [00:02<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 1: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 32.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 5.25 GiB is allocated by PyTorch, and 102.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 3/3 [00:02<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 2: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 22.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.47 GiB memory in use. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 60.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([2, 4, 64000]), Clean: torch.Size([2, 1, 64000]), Video: torch.Size([2, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss 0.0000 | Val Loss 30.9902\n",
      "New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  33%|███▎      | 1/3 [00:01<00:02,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 0: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 32.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 81.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  67%|██████▋   | 2/3 [00:01<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 1: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 262.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 2: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 2.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.49 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 73.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([2, 4, 64000]), Clean: torch.Size([2, 1, 64000]), Video: torch.Size([2, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss 0.0000 | Val Loss 33.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  33%|███▎      | 1/3 [00:01<00:03,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 0: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 32.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 81.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  67%|██████▋   | 2/3 [00:01<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 1: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 262.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 2: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 2.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.49 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 73.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([2, 4, 64000]), Clean: torch.Size([2, 1, 64000]), Video: torch.Size([2, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss 0.0000 | Val Loss 53.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 0: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 32.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 81.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100:  67%|██████▋   | 2/3 [00:01<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 1: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 262.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 2: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 2.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.49 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 73.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([2, 4, 64000]), Clean: torch.Size([2, 1, 64000]), Video: torch.Size([2, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Train Loss 0.0000 | Val Loss 36.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100:  33%|███▎      | 1/3 [00:01<00:02,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 0: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 32.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 81.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 1: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 262.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 2: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 2.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.49 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 73.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([2, 4, 64000]), Clean: torch.Size([2, 1, 64000]), Video: torch.Size([2, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Train Loss 0.0000 | Val Loss 32.8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100:  33%|███▎      | 1/3 [00:01<00:02,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 0: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 32.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 81.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100:  67%|██████▋   | 2/3 [00:01<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 1: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 262.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 2: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 2.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.49 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 73.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([2, 4, 64000]), Clean: torch.Size([2, 1, 64000]), Video: torch.Size([2, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Train Loss 0.0000 | Val Loss 31.5824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 0: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 32.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 81.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100:  67%|██████▋   | 2/3 [00:01<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 1: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 262.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 2: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 2.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.49 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 73.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([2, 4, 64000]), Clean: torch.Size([2, 1, 64000]), Video: torch.Size([2, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Train Loss 0.0000 | Val Loss 31.5458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100:  33%|███▎      | 1/3 [00:01<00:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 0: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 32.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 81.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100:  67%|██████▋   | 2/3 [00:01<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 1: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 262.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 3/3 [00:01<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 2: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 2.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.49 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 73.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([2, 4, 64000]), Clean: torch.Size([2, 1, 64000]), Video: torch.Size([2, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Train Loss 0.0000 | Val Loss 31.4672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100:  33%|███▎      | 1/3 [00:01<00:02,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 0: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 32.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 5.26 GiB is allocated by PyTorch, and 81.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100:  67%|██████▋   | 2/3 [00:01<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 1: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 262.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 158.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([4, 4, 64000]), Clean: torch.Size([4, 1, 64000]), Video: torch.Size([4, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 3/3 [00:02<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at batch 2: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 5.67 GiB of which 2.44 MiB is free. Process 260556 has 164.00 MiB memory in use. Including non-PyTorch memory, this process has 5.49 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 73.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Mixed: torch.Size([2, 4, 64000]), Clean: torch.Size([2, 1, 64000]), Video: torch.Size([2, 3, 100, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "# 3. Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Progress Bar\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for i, (mixed, clean, video) in enumerate(pbar):\n",
    "        try:\n",
    "            # Move to GPU\n",
    "            mixed = mixed.to(DEVICE)   # [B, 4, T]\n",
    "            clean = clean.to(DEVICE)   # [B, 1, T]\n",
    "            video = video.to(DEVICE)   # [B, 3, T, H, W]\n",
    "            \n",
    "            # Debug shapes and audio scaling on first iteration\n",
    "            if i == 0 and epoch == 0:\n",
    "                print(f\"Mixed shape: {mixed.shape}, Clean shape: {clean.shape}, Video shape: {video.shape}\")\n",
    "                print(f\"Audio Max: {mixed.abs().max():.4f}, Audio Min: {mixed.abs().min():.4f}\")\n",
    "                if mixed.abs().max() == 0.0:\n",
    "                    print(\"WARNING: Audio is silent! Check your data loading.\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            estimate = model(mixed, video) # [B, 1, T]\n",
    "            \n",
    "            # Verify output shape matches input\n",
    "            if estimate.shape[-1] != clean.shape[-1]:\n",
    "                print(f\"Shape mismatch! Estimate: {estimate.shape}, Clean: {clean.shape}\")\n",
    "                # Trim to minimum length\n",
    "                min_len = min(estimate.shape[-1], clean.shape[-1])\n",
    "                estimate = estimate[..., :min_len]\n",
    "                clean = clean[..., :min_len]\n",
    "            \n",
    "            # Calculate Loss (Squeeze channels to match [B, T])\n",
    "            loss = si_snr_loss(estimate.squeeze(1), clean.squeeze(1))\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Clipping (Prevents crashes in TCNs)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.2f}\"})\n",
    "            \n",
    "            # Clear GPU cache periodically\n",
    "            if i % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error at batch {i}: {str(e)}\")\n",
    "            print(f\"Mixed: {mixed.shape}, Clean: {clean.shape}, Video: {video.shape}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # 4. Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for mixed, clean, video in val_loader:\n",
    "            try:\n",
    "                mixed, clean, video = mixed.to(DEVICE), clean.to(DEVICE), video.to(DEVICE)\n",
    "                estimate = model(mixed, video)\n",
    "                \n",
    "                # Handle shape mismatch in validation too\n",
    "                if estimate.shape[-1] != clean.shape[-1]:\n",
    "                    min_len = min(estimate.shape[-1], clean.shape[-1])\n",
    "                    estimate = estimate[..., :min_len]\n",
    "                    clean = clean[..., :min_len]\n",
    "                \n",
    "                loss = si_snr_loss(estimate.squeeze(1), clean.squeeze(1))\n",
    "                val_loss += loss.item()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Validation error: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else float('inf')\n",
    "    \n",
    "    # 5. Logging & Saving\n",
    "    print(f\"\\nEpoch {epoch+1}: Train Loss {avg_train_loss:.4f} | Val Loss {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Save Last\n",
    "    torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/last.pth\")\n",
    "    \n",
    "    # Save Best\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
    "        print(\"New Best Model Saved!\")\n",
    "    \n",
    "    # Clear cache after each epoch\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a04ac",
   "metadata": {},
   "source": [
    "## Face Cropping Test\n",
    "\n",
    "Visualize a sample from the dataset to verify that face cropping is working correctly. This will load one sample and display multiple frames to show the face region extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test dataset with full frame loading\n",
    "print(\"Loading dataset sample to test full frame loading...\")\n",
    "test_csv = \"/run/media/neuronetix/BACKUP/Dataset/VOX/manual/dev/multich/train.csv\"\n",
    "test_ds = IsoNetDataset(test_csv, max_samples=5, video_size=(224, 224))\n",
    "\n",
    "# Load first sample\n",
    "mixed, clean, video = test_ds[0]\n",
    "\n",
    "print(\"\\n--- Tensor Shapes ---\")\n",
    "print(f\"Mixed Audio: {mixed.shape}  (Expected: [4, 64000])\")\n",
    "print(f\"Clean Audio: {clean.shape}  (Expected: [1, 64000])\")\n",
    "print(f\"Video:       {video.shape}  (Expected: [3, 100, H, W])\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Full Frame Verification: 10 Sample Frames (224x224)', fontsize=16)\n",
    "\n",
    "# Sample 10 frames evenly across the 4-second clip\n",
    "sample_frames = np.linspace(0, video.shape[1]-1, 10, dtype=int)\n",
    "\n",
    "for idx, frame_num in enumerate(sample_frames):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    \n",
    "    # Permute from [C, T, H, W] -> [H, W, C] for display\n",
    "    frame_tensor = video[:, frame_num, :, :].permute(1, 2, 0)\n",
    "    axes[row, col].imshow(frame_tensor.numpy())\n",
    "    axes[row, col].set_title(f'Frame {frame_num}', fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"full_frame_check.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Full frame loading test complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
